{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb4c5c01",
   "metadata": {},
   "source": [
    "# 0. Environment Setup with pip\n",
    "\n",
    "This command installs all required libraries in one step for a macOS-based development environment. It includes deep learning (Keras/TensorFlow), machine learning (XGBoost), hyperparameter optimization (Hyperopt, Optuna), experiment tracking (MLflow), and model wrapping (Scikeras)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5f1cc4a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Ignoring invalid distribution ~ensorflow (/opt/anaconda3/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~ensorflow (/opt/anaconda3/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~ensorflow (/opt/anaconda3/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -q --disable-pip-version-check \\\n",
    "    keras \\\n",
    "    tensorflow-macos \\\n",
    "    tensorflow-metal \\\n",
    "    xgboost \\\n",
    "    hyperopt \\\n",
    "    mlflow \\\n",
    "    optuna \\\n",
    "    scikeras \\\n",
    "    gdown"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e626de0a",
   "metadata": {},
   "source": [
    "# 1. Import libraries\n",
    "\n",
    "This section imports all necessary Python libraries and modules required for data handling, model building, evaluation, optimization, and visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a0ed144",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Standard Library ---\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# --- Progress Tracking ---\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --- Data Handling ---\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# --- Visualization ---\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# --- Scikit-learn ---\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder, RobustScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "# --- Model Persistence ---\n",
    "import joblib\n",
    "\n",
    "# --- MLflow for Experiment Tracking ---\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "import mlflow.keras\n",
    "from mlflow.models.signature import infer_signature\n",
    "\n",
    "# --- XGBoost ---\n",
    "import xgboost as xgb\n",
    "\n",
    "# --- Keras / TensorFlow ---\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, Model, load_model\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "# --- SciKeras (optional Keras wrapper for sklearn) ---\n",
    "from scikeras.wrappers import KerasRegressor\n",
    "\n",
    "# --- Hyperparameter Optimization ---\n",
    "from hyperopt import fmin, tpe, hp, Trials, STATUS_OK\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a14d61a1",
   "metadata": {},
   "source": [
    "This snippet ensures the script can access project-level configuration regardless of execution context. It dynamically sets the project root, adjusts the system path for imports, and loads configuration constants needed for forecasting workflows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e5c59fc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features: ['store_nbr', 'item_nbr', 'onpromotion', 'day_of_week', 'month', 'unit_sales_7d_avg', 'lag_1', 'lag_7', 'rolling_mean_7']\n",
      "Sequence Length: 90\n",
      "Target: unit_sales\n",
      "Cutoff Date: 2013-12-31 00:00:00\n",
      "Forecast End: 2014-03-31\n",
      "Hyperopt Space: {'max_depth': [3, 4, 5, 6], 'learning_rate_range': (0.01, 0.1, 0.2, 0.3), 'n_estimators': [20, 50, 100]}\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Get project root one level up from current working directory (for Jupyter use)\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "\n",
    "# Add project root to Python path if not already included\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n",
    "\n",
    "# Import constants from Streamlit app configuration\n",
    "import importlib\n",
    "import app.config as cfg\n",
    "importlib.reload(cfg)\n",
    "\n",
    "import model.model_utils as model_utils\n",
    "\n",
    "\n",
    "\n",
    "def main():\n",
    "    print(\"Features:\", cfg.FEATURES)\n",
    "    print(\"Sequence Length:\", cfg.SEQ_LEN)\n",
    "    print(\"Target:\", cfg.TARGET)\n",
    "    print(\"Cutoff Date:\", cfg.CUTOFF_DATE)\n",
    "    print(\"Forecast End:\", cfg.FORECAST_END)\n",
    "    print(\"Hyperopt Space:\", cfg.HYPEROPT_SPACE)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79263f93",
   "metadata": {},
   "source": [
    "# 2. Import Data\n",
    "\n",
    "Import data prepared in the first notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f2a7f30c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(\"../data/preprocessed_data/train_guayas_prepared.csv\", parse_dates=['date'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb82d5b3",
   "metadata": {},
   "source": [
    "# 3. Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08aab94d",
   "metadata": {},
   "source": [
    "## 3.1 Create feature and set dtypes\n",
    "\n",
    "This section performs essential preprocessing tasks, such as datetime conversion, label encoding, memory optimization, and feature engineering. It includes lag features and rolling statistics, which are commonly used in time series forecasting models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bd5da0ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        date  unit_sales  lag_1  lag_7  rolling_mean_7\n",
      "0 2013-01-09         2.0    0.0    0.0        0.000000\n",
      "1 2013-01-10         0.0    2.0    0.0        0.285714\n",
      "2 2013-01-11         0.0    0.0    0.0        0.285714\n",
      "3 2013-01-12         2.0    0.0    0.0        0.285714\n",
      "4 2013-01-13         0.0    2.0    0.0        0.571429\n",
      "5 2013-01-14         0.0    0.0    0.0        0.571429\n",
      "6 2013-01-15         0.0    0.0    0.0        0.571429\n",
      "7 2013-01-16         1.0    0.0    2.0        0.571429\n",
      "8 2013-01-17         2.0    1.0    0.0        0.428571\n",
      "9 2013-01-18         0.0    2.0    0.0        0.714286\n"
     ]
    }
   ],
   "source": [
    "# 1. Convert the 'date' column to datetime\n",
    "df_train['date'] = pd.to_datetime(df_train['date'])\n",
    "\n",
    "# 2. Label-encode family on train, then apply same encoder to test\n",
    "le = LabelEncoder()\n",
    "df_train['family_code'] = le.fit_transform(df_train['family'])\n",
    "df_train ['family_code'] = le.transform   (df_train ['family']).astype('uint8')\n",
    "\n",
    "# 3. Downcast integer columns to reduce memory usage\n",
    "df_train['store_nbr']   = df_train['store_nbr'].astype('uint8')\n",
    "df_train['item_nbr']    = df_train['item_nbr'].astype('uint32')\n",
    "df_train['day_of_week'] = df_train['day_of_week'].astype('uint8')\n",
    "df_train['month']       = df_train['month'].astype('uint8')\n",
    "df_train['year']        = df_train['year'].astype('uint16')\n",
    "df_train['onpromotion'] = df_train['onpromotion'].astype('bool')\n",
    "\n",
    "# 4. Downcast other numeric columns where possible\n",
    "df_train['unit_sales']       = pd.to_numeric(df_train['unit_sales'],       downcast='float')\n",
    "df_train['unit_sales_7d_avg'] = pd.to_numeric(df_train['unit_sales_7d_avg'], downcast='float')\n",
    "\n",
    "# 5. Lag features\n",
    "df_train['lag_1'] = df_train['unit_sales'].shift(1)\n",
    "df_train['lag_7'] = df_train['unit_sales'].shift(7)\n",
    "\n",
    "# 6. Rolling-Window-Features\n",
    "df_train['rolling_mean_7']     = df_train['unit_sales'].shift(1).rolling(window=7).mean()\n",
    "df_train['unit_sales_7d_avg']  = df_train['unit_sales'].shift(1).rolling(window=7).mean()\n",
    "\n",
    "# 7. Drop rows with NaN values\n",
    "df_train = df_train.dropna(subset=['lag_1', 'lag_7', 'rolling_mean_7']).reset_index(drop=True)\n",
    "\n",
    "# 8. Drop unnecessary columns\n",
    "int_cols = df_train.select_dtypes(include='int').columns\n",
    "df_train[int_cols] = df_train[int_cols].astype(\"float16\")\n",
    "\n",
    "print(df_train.filter(['date','unit_sales','lag_1','lag_7','rolling_mean_7']).head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8274c78",
   "metadata": {},
   "source": [
    "## 3.2 Save the dataset\n",
    "\n",
    "After the dataset is prepared and further features were included, the dataset has to be saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a10fa834",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.to_csv(\"../data/preprocessed_data/train_guayas_model_ready.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48c3a2af",
   "metadata": {},
   "source": [
    "## 3.3 Filter for best Store-Item-combination\n",
    "\n",
    "This section filters the dataset to include only data from 2013 and computes basic statistics per store and item combination. It selects only combinations with sufficient data coverage and calculates a custom score based on mean and standard deviation of sales. The best-performing combination is selected to serve as the focus for subsequent model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ec788b07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model for store 24 and item 220435\n"
     ]
    }
   ],
   "source": [
    "# Filter 2013 data only\n",
    "df_2013 = df_train[df_train[\"date\"].dt.year == 2013]\n",
    "\n",
    "# Group by store and item to compute statistics\n",
    "stats = df_2013.groupby([\"store_nbr\", \"item_nbr\"]).agg(\n",
    "    count_days=(\"unit_sales\", \"count\"),\n",
    "    mean_sales=(\"unit_sales\", \"mean\"),\n",
    "    std_sales=(\"unit_sales\", \"std\"),\n",
    "    positive_days=(\"unit_sales\", lambda x: (x > 0).sum())\n",
    ").reset_index()\n",
    "\n",
    "# Filter only combinations with sufficient data\n",
    "stats = stats[stats[\"count_days\"] >= 300]\n",
    "\n",
    "# Compute a custom score: mean / std\n",
    "stats[\"score\"] = stats[\"mean_sales\"] / (stats[\"std_sales\"] + 1e-5)\n",
    "\n",
    "# Get the best combination\n",
    "best_combo = stats.sort_values(\"score\", ascending=False).iloc[0]\n",
    "store_id = int(best_combo[\"store_nbr\"])\n",
    "item_id = int(best_combo[\"item_nbr\"])\n",
    "\n",
    "# Filter data for selected combination\n",
    "print(f\"Training model for store {store_id} and item {item_id}\")\n",
    "df = df_train[(df_train[\"store_nbr\"] == store_id) & (df_train[\"item_nbr\"] == item_id)].copy()\n",
    "df_train = df.sort_values(\"date\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e4cf796",
   "metadata": {},
   "source": [
    "## 3.4 Train/Test-Split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84772adc",
   "metadata": {},
   "source": [
    "This section splits the dataset into training and test sets based on a predefined `CUTOFF_DATE`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a9e9f683",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/Test Split with Cutoff from config\n",
    "train_df = df_train.loc[df_train[\"date\"] <= cfg.CUTOFF_DATE].copy()\n",
    "test_df  = df_train.loc[df_train[\"date\"] >  cfg.CUTOFF_DATE].copy()\n",
    "\n",
    "non_numeric_cols = df_train.select_dtypes(include=[\"datetime64[ns]\", \"datetime64\", \"object\"]).columns\n",
    "\n",
    "X_train = train_df.drop(columns=non_numeric_cols).drop(columns=[cfg.TARGET])\n",
    "y_train = np.log1p(train_df[cfg.TARGET])\n",
    "X_test  = test_df.drop(columns=non_numeric_cols).drop(columns=[cfg.TARGET])\n",
    "y_test  = np.log1p(test_df[cfg.TARGET])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9025b5e",
   "metadata": {},
   "source": [
    "# 4. XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31e852c5",
   "metadata": {},
   "source": [
    "## 4.1 Hyperparameter-Optimized XGBoost Model (Hyperopt)\n",
    "\n",
    "This code performs **automated hyperparameter tuning** and training of an XGBoost model to forecast `unit_sales` for a given store and item using **Hyperopt** and **MLflow**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f77377d",
   "metadata": {},
   "source": [
    "### Purpose\n",
    "To find the best-performing XGBoost model configuration for each store/item combination through hyperparameter optimization. The goal is to minimize forecast error using a data-driven, repeatable search strategy.\n",
    "\n",
    "### What the code does\n",
    "- Defines a **hyperparameter search space** for key XGBoost settings (e.g., `max_depth`, `learning_rate`, `n_estimators`)\n",
    "- Uses **Hyperopt** to explore this space with a loss function based on `MAE`\n",
    "- Trains 25 model variations (`max_evals=25`) on the training set\n",
    "- Selects the best model and evaluates it on the test set\n",
    "- Logs the selected model, parameters, and metrics (`MAE`, `RÂ²`) to **MLflow**\n",
    "- Saves the final model to disk for reuse or deployment\n",
    "\n",
    "This setup supports consistent, scalable model tuning and tracking, enabling per-store/item optimization for time series forecasting tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b52fdec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [14:10<00:00, 34.00s/trial, best loss: 1.0784331925977066]\n",
      "âœ… Best MAE: 1.0784 | RÂ²: 0.5962\n",
      "âœ… Modell gespeichert unter: /Users/jennypetschke/VS_Code_Projects/retail_demand_analysis/model/xgb/archive/xgb_hyperopt_global_model.pkl\n"
     ]
    }
   ],
   "source": [
    "# --- Imports ---\n",
    "import app.config as cfg  # Optional config file (if available)\n",
    "\n",
    "# -------------------- Setup --------------------\n",
    "# Define paths\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "data_path = os.path.join(project_root, cfg.PREPARED_DATA_FILE)\n",
    "mlruns_path = os.path.join(project_root, \"mlruns\", \"xgb_hyperopt\")\n",
    "model_output_path = Path(project_root, cfg.XGB_ARCHIVE_DIR) / \"xgb_hyperopt_global_model.pkl\"\n",
    "\n",
    "# Ensure necessary directories exist\n",
    "os.makedirs(mlruns_path, exist_ok=True)\n",
    "os.makedirs(model_output_path.parent, exist_ok=True)\n",
    "\n",
    "# -------------------- Load & preprocess data --------------------\n",
    "df = pd.read_csv(data_path, parse_dates=[\"date\"])\n",
    "df = df[df[\"unit_sales\"] >= 0]\n",
    "df = df[df[\"date\"] <= cfg.CUTOFF_DATE]\n",
    "\n",
    "# Define features and target\n",
    "FEATURE_COLS = [\n",
    "    'store_nbr', 'item_nbr', 'onpromotion', 'month',\n",
    "    'unit_sales_7d_avg', 'family_code', 'lag_1', 'lag_7', 'rolling_mean_7'\n",
    "]\n",
    "TARGET = 'unit_sales'\n",
    "\n",
    "# Feature matrix and target (log1p transformed)\n",
    "X = df[FEATURE_COLS].astype(\"float32\")\n",
    "y = np.log1p(df[TARGET].values)\n",
    "\n",
    "# Split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "X_test = X_test[FEATURE_COLS]  # Ensure proper order\n",
    "\n",
    "# -------------------- Define hyperparameter search space --------------------\n",
    "space = {\n",
    "    'max_depth': hp.choice('max_depth', range(3, 10)),\n",
    "    'learning_rate': hp.uniform('learning_rate', 0.01, 0.3),\n",
    "    'n_estimators': hp.choice('n_estimators', range(50, 301, 50)),\n",
    "    'subsample': hp.uniform('subsample', 0.5, 1.0),\n",
    "    'colsample_bytree': hp.uniform('colsample_bytree', 0.5, 1.0),\n",
    "}\n",
    "\n",
    "# -------------------- Objective function for optimization --------------------\n",
    "def objective(params):\n",
    "    model = xgb.XGBRegressor(random_state=42, **params)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = np.expm1(model.predict(X_test))  # Reverse log1p transform\n",
    "    score = mean_absolute_error(np.expm1(y_test), y_pred)  # Evaluate in original scale\n",
    "    return {'loss': score, 'status': STATUS_OK, 'model': model}\n",
    "\n",
    "# -------------------- Run Hyperopt optimization --------------------\n",
    "trials = Trials()\n",
    "best = fmin(fn=objective, space=space, algo=tpe.suggest, max_evals=25, trials=trials)\n",
    "\n",
    "# Retrieve best model from trials\n",
    "best_model = trials.best_trial['result']['model']\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred_log = best_model.predict(X_test)\n",
    "y_pred = np.expm1(y_pred_log)\n",
    "y_true = np.expm1(y_test)\n",
    "\n",
    "mae = mean_absolute_error(y_true, y_pred)\n",
    "r2 = r2_score(y_true, y_pred)\n",
    "\n",
    "# -------------------- MLflow logging --------------------\n",
    "mlflow.set_tracking_uri(f\"file://{mlruns_path}\")\n",
    "mlflow.set_experiment(\"store_item_sales_forecast_hyperopt\")\n",
    "\n",
    "with mlflow.start_run():\n",
    "    mlflow.log_param(\"model_type\", \"XGBoost + Hyperopt\")\n",
    "    mlflow.log_param(\"feature_cols\", FEATURE_COLS)\n",
    "    mlflow.log_params(best)\n",
    "    mlflow.log_metric(\"mae\", mae)\n",
    "    mlflow.log_metric(\"r2_score\", r2)\n",
    "\n",
    "    # Save model to disk\n",
    "    joblib.dump(best_model, model_output_path)\n",
    "    mlflow.log_artifact(str(model_output_path), artifact_path=\"model\")\n",
    "\n",
    "    # Log model with schema signature and sample input\n",
    "    signature = infer_signature(X_train, best_model.predict(X_train))\n",
    "    mlflow.sklearn.log_model(\n",
    "        sk_model=best_model,\n",
    "        artifact_path=\"model_mlflow\",\n",
    "        signature=signature,\n",
    "        input_example=X_train.head(2)\n",
    "    )\n",
    "\n",
    "    # Forecast vs. actual plot\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.plot(y_true[:300], label=\"Actual\")\n",
    "    plt.plot(y_pred[:300], label=\"Forecast\", linestyle=\"--\")\n",
    "    plt.title(\"Forecast vs Actual\")\n",
    "    plt.xlabel(\"Index\")\n",
    "    plt.ylabel(\"Unit Sales\")\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    forecast_path = \"forecast_vs_actual_hyperopt.png\"\n",
    "    plt.savefig(forecast_path)\n",
    "    mlflow.log_artifact(forecast_path)\n",
    "    plt.close()\n",
    "\n",
    "    print(f\"âœ… Best MAE: {mae:.4f} | RÂ²: {r2:.4f}\")\n",
    "    print(f\"âœ… Model saved to: {model_output_path}\")\n",
    "\n",
    "# --- End MLflow run if still active ---\n",
    "if mlflow.active_run():\n",
    "    mlflow.end_run()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01791060",
   "metadata": {},
   "source": [
    "## 4.2 XGBoost Regression Model (Global)\n",
    "\n",
    "The following Code is the XGBoost regression model for predicting unit sales using engineered features, evaluate its performance, and log everything with MLflow (including metrics, model, and visualizations)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8a4c761",
   "metadata": {},
   "source": [
    "### Purpose  \n",
    "The purpose of the following code is to train a **global XGBoost model** that predicts unit sales based on multiple engineered features. It aims to capture general sales patterns across all stores and items and evaluate the modelâ€™s performance using robust metrics. The full pipeline is tracked using **MLflow** for transparency and reproducibility.\n",
    "\n",
    "### What the function does\n",
    "- Loads and filters historical sales data up to a defined `CUTOFF_DATE`\n",
    "- Selects relevant features and applies a log transformation to `unit_sales`\n",
    "- Splits the dataset into training and testing subsets\n",
    "- Trains an XGBoost regression model on the processed data\n",
    "- Predicts test values and applies inverse transformation using `expm1`\n",
    "- Evaluates model performance using `MAE` and `RÂ²`\n",
    "- Logs model parameters, evaluation metrics, and plots using **MLflow**\n",
    "- Saves the trained model and feature metadata to disk\n",
    "- Generates and logs:\n",
    "  - A time series plot of unit sales over time\n",
    "  - A forecast vs. actual plot to visualize prediction accuracy\n",
    "\n",
    "This pipeline supports fast experimentation and ensures results are reproducible, interpretable, and ready for deployment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a6460f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Global model saved to /Users/jennypetschke/VS_Code_Projects/retail_demand_analysis/model/xgb/archive/xgb_global.pkl\n",
      "ðŸ“Š MAE: 1.0521 | R2: 0.6061\n"
     ]
    }
   ],
   "source": [
    "# --- End any active MLflow run ---\n",
    "if mlflow.active_run():  # If the code was interrupted earlier and there's an active MLflow run\n",
    "    mlflow.end_run()     # Properly end the run to avoid conflicts\n",
    "\n",
    "# --- Feature columns used for training ---\n",
    "FEATURE_COLS = [\n",
    "    'store_nbr', 'item_nbr', 'onpromotion',\n",
    "    'day_of_week', 'month', 'year', 'unit_sales_7d_avg',\n",
    "    'family_code', 'lag_1', 'lag_7', 'rolling_mean_7'\n",
    "]\n",
    "\n",
    "# --- Load and filter dataset ---\n",
    "df = pd.read_csv(os.path.join(project_root, cfg.PREPARED_DATA_FILE), parse_dates=[\"date\"])\n",
    "df = df[df['unit_sales'] >= 0]  # Remove negative sales\n",
    "df = df[df['date'] <= cfg.CUTOFF_DATE]  # Only keep data up to cutoff date\n",
    "\n",
    "# --- Define features and log-transformed target ---\n",
    "X = df[FEATURE_COLS].astype(\"float64\")\n",
    "y = np.log1p(df['unit_sales'])  # Apply log1p to stabilize skewed distribution\n",
    "\n",
    "# --- Train/test split ---\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "import xgboost as xgb\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# --- Train XGBoost model ---\n",
    "model = xgb.XGBRegressor(random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# --- Make predictions and revert log1p transform ---\n",
    "y_pred_log = model.predict(X_test)\n",
    "y_pred = np.expm1(y_pred_log)  # Convert back to original scale\n",
    "y_true = np.expm1(y_test)\n",
    "\n",
    "# --- Evaluate model performance ---\n",
    "mae = mean_absolute_error(y_true, y_pred)\n",
    "r2 = r2_score(y_true, y_pred)\n",
    "\n",
    "# --- Set up MLflow tracking ---\n",
    "mlflow_tracking_dir_xgb_global = os.path.join(project_root, \"mlruns\", \"xgb_global\")\n",
    "if not os.path.exists(mlflow_tracking_dir_xgb_global):\n",
    "    os.makedirs(mlflow_tracking_dir_xgb_global, exist_ok=True)\n",
    "\n",
    "mlflow.set_tracking_uri(f\"file://{mlflow_tracking_dir_xgb_global}\")\n",
    "mlflow.set_experiment(\"global_xgb_sales_forecast\")\n",
    "\n",
    "# --- Start MLflow run ---\n",
    "with mlflow.start_run():\n",
    "    # Log parameters and evaluation metrics\n",
    "    mlflow.log_param(\"model_type\", \"xgb_global\")\n",
    "    mlflow.log_param(\"features\", FEATURE_COLS)\n",
    "    mlflow.log_metric(\"mae\", mae)\n",
    "    mlflow.log_metric(\"r2_score\", r2)\n",
    "\n",
    "    # Save trained model and feature list\n",
    "    out_path = Path(project_root, cfg.XGB_ARCHIVE_DIR) / cfg.GLOBAL_XGB_MODEL\n",
    "    out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    joblib.dump({\"model\": model, \"features\": FEATURE_COLS}, out_path)\n",
    "    mlflow.log_artifact(str(out_path), artifact_path=\"model\")\n",
    "\n",
    "    print(f\"âœ… Global model saved to {out_path}\")\n",
    "    print(f\"ðŸ“Š MAE: {mae:.4f} | R2: {r2:.4f}\")\n",
    "\n",
    "    # --- Plot 1: Time series of original sales data ---\n",
    "    import matplotlib.pyplot as plt\n",
    "    df_sorted = df.sort_values(\"date\")\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.plot(df_sorted[\"date\"], df_sorted[\"unit_sales\"], label=\"unit_sales\", alpha=0.8)\n",
    "    plt.title(\"Unit Sales over Time\")\n",
    "    plt.xlabel(\"Date\")\n",
    "    plt.ylabel(\"Unit Sales\")\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    time_series_path = \"unit_sales_over_time.png\"\n",
    "    plt.savefig(time_series_path)\n",
    "    plt.close()\n",
    "    mlflow.log_artifact(time_series_path)\n",
    "\n",
    "    # --- Plot 2: Forecast vs actual (sample) ---\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    sample_size = min(300, len(y_true))\n",
    "    plt.plot(y_true[:sample_size], label=\"Actual\", linewidth=2)\n",
    "    plt.plot(y_pred[:sample_size], label=\"Forecast\", linestyle=\"--\")\n",
    "    plt.title(\"Forecast vs. Actual (Test Set)\")\n",
    "    plt.xlabel(\"Sample\")\n",
    "    plt.ylabel(\"Unit Sales\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    forecast_path = \"forecast_vs_actual.png\"\n",
    "    plt.savefig(forecast_path)\n",
    "    plt.close()\n",
    "    mlflow.log_artifact(forecast_path)\n",
    "\n",
    "# --- Clean up: Ensure MLflow run is ended properly ---\n",
    "if mlflow.active_run():\n",
    "    mlflow.end_run()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0bc253b",
   "metadata": {},
   "source": [
    "# 5. LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "346632a7",
   "metadata": {},
   "source": [
    "## 5.1 LSTM Sequence-to-Sequence Forecasting Model (Global)\n",
    "\n",
    "This function trains a **global LSTM Seq2Seq model** for multi-step time series forecasting, such as predicting future `unit_sales`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeec43e6",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Purpose\n",
    "The purpose of the following Code is to learn general sales patterns from historical data and forecast multiple future time steps in a single forward pass. The model is trained on multiple features, like aggregated data, but also on store_nbr and item_nbr.\n",
    "\n",
    "### What the function does\n",
    "- Loads and filters time series data up to a defined `CUTOFF_DATE`\n",
    "- Scales `unit_sales` using MinMaxScaler\n",
    "- Prepares input/output sequences using a sliding window\n",
    "- Builds a Sequence-to-Sequence LSTM model (Encoderâ€“Decoder)\n",
    "- Trains the model with callbacks (`EarlyStopping`, `ReduceLROnPlateau`)\n",
    "- Logs all key parameters, metrics, and artifacts using **MLflow**\n",
    "- Saves both the trained model and the scaler for future use\n",
    "\n",
    "This setup ensures that the entire training process is tracked, reproducible, and ready for deployment or experimentation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e579437d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'app'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 15\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptimizers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Adam\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mimportlib\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mapp\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfig\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mcfg\u001b[39;00m\n\u001b[1;32m     16\u001b[0m importlib\u001b[38;5;241m.\u001b[39mreload(cfg)\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# --- Parameter ---\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'app'"
     ]
    }
   ],
   "source": [
    "# -- Robust Forecasting with log1p Target Transformation --\n",
    "import importlib\n",
    "import app.config as cfg\n",
    "importlib.reload(cfg)\n",
    "\n",
    "# --- Parameters ---\n",
    "SEQ_LEN = 90  # Length of input/output sequences for the LSTM model\n",
    "LSTM_UNITS = 8  # Number of LSTM units\n",
    "EPOCHS = 100  # Maximum number of training epochs\n",
    "BATCH_SIZE = 16  # Training batch size\n",
    "MAX_SAMPLES = 800  # Max number of samples used for training to reduce memory usage\n",
    "dtype = \"float32\"  # Float type for model inputs\n",
    "FEATURES = ['onpromotion', 'lag_1', 'rolling_mean_7']  # Feature columns\n",
    "\n",
    "# --- Paths ---\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "data_path = os.path.join(project_root, cfg.PREPARED_DATA_FILE)\n",
    "mlflow_tracking_dir = os.path.join(project_root, cfg.MLFLOW_LSTM_SEQ)\n",
    "model_path = Path(project_root) / cfg.LSTM_ARCHIVE_DIR / cfg.LSTM_GLOBAL_MODEL\n",
    "scaler_path = Path(project_root) / cfg.SCALER_ARCHIVE_DIR / cfg.LSTM_GLOBAL_SCALER\n",
    "\n",
    "# --- Load data ---\n",
    "df = pd.read_csv(data_path, usecols=FEATURES + ['unit_sales', 'date'], parse_dates=[\"date\"])\n",
    "df = df[(df[\"unit_sales\"] >= 0) & (df[\"date\"] <= cfg.CUTOFF_DATE)].sort_values(\"date\").reset_index(drop=True)\n",
    "df[\"target\"] = np.log1p(df[\"unit_sales\"])  # Apply log1p transform to stabilize variance\n",
    "\n",
    "# --- Feature Scaling ---\n",
    "feature_scaler = MinMaxScaler()\n",
    "target_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "X_scaled = feature_scaler.fit_transform(df[FEATURES]).astype(dtype)\n",
    "y_scaled = target_scaler.fit_transform(df[[\"target\"]]).astype(dtype)\n",
    "\n",
    "# --- Prepare sequence data for encoder-decoder LSTM ---\n",
    "def make_seq2seq(X, y, n_in, n_out):\n",
    "    X_seq, y_seq = [], []\n",
    "    for i in range(len(X) - n_in - n_out + 1):\n",
    "        X_seq.append(X[i:i+n_in])\n",
    "        y_seq.append(y[i+n_in:i+n_in+n_out, 0])\n",
    "    return np.array(X_seq), np.array(y_seq)\n",
    "\n",
    "X_raw, y_raw = make_seq2seq(X_scaled, y_scaled, SEQ_LEN, SEQ_LEN)\n",
    "\n",
    "X_enc = X_raw.astype(dtype)  # Encoder input\n",
    "X_dec = np.zeros((len(X_raw), SEQ_LEN, 1), dtype=dtype)  # Decoder input\n",
    "X_dec[:, 1:, 0] = y_raw[:, :-1]  # Shifted target as decoder input\n",
    "X_dec[:, 0, 0] = X_enc[:, -1, 0]  # First input: last feature from encoder\n",
    "\n",
    "y_seq = y_raw.reshape(len(y_raw), SEQ_LEN, 1).astype(dtype)  # Decoder output\n",
    "\n",
    "# Reduce sample size if necessary\n",
    "if len(X_enc) > MAX_SAMPLES:\n",
    "    X_enc = X_enc[-MAX_SAMPLES:]\n",
    "    X_dec = X_dec[-MAX_SAMPLES:]\n",
    "    y_seq = y_seq[-MAX_SAMPLES:]\n",
    "\n",
    "# --- Build LSTM Seq2Seq model ---\n",
    "input_dim = X_enc.shape[2]\n",
    "\n",
    "# Encoder\n",
    "enc_inputs = Input(shape=(SEQ_LEN, input_dim), name=\"encoder_input\")\n",
    "_, state_h, state_c = LSTM(LSTM_UNITS, return_state=True)(enc_inputs)\n",
    "\n",
    "# Decoder\n",
    "dec_inputs = Input(shape=(SEQ_LEN, 1), name=\"decoder_input\")\n",
    "dec_outputs = LSTM(LSTM_UNITS, return_sequences=True)(dec_inputs, initial_state=[state_h, state_c])\n",
    "outputs = Dense(1)(dec_outputs)\n",
    "\n",
    "# Compile model\n",
    "model = Model([enc_inputs, dec_inputs], outputs)\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss=\"mae\")\n",
    "\n",
    "# --- MLflow setup ---\n",
    "mlflow.set_tracking_uri(f\"file://{mlflow_tracking_dir}\")\n",
    "mlflow.set_experiment(\"lstm_seq2seq_log1p\")\n",
    "\n",
    "if mlflow.active_run():\n",
    "    mlflow.end_run()  # End any active run\n",
    "\n",
    "with mlflow.start_run():\n",
    "    # Log model parameters\n",
    "    mlflow.log_params({\n",
    "        \"seq_len\": SEQ_LEN,\n",
    "        \"lstm_units\": LSTM_UNITS,\n",
    "        \"epochs\": EPOCHS,\n",
    "        \"batch_size\": BATCH_SIZE,\n",
    "        \"features\": FEATURES\n",
    "    })\n",
    "\n",
    "    # Train the model\n",
    "    history = model.fit([X_enc, X_dec], y_seq, epochs=EPOCHS, batch_size=BATCH_SIZE,\n",
    "                        callbacks=[\n",
    "                            EarlyStopping(monitor=\"loss\", patience=3, restore_best_weights=True),\n",
    "                            ReduceLROnPlateau(monitor=\"loss\", factor=0.5, patience=2)\n",
    "                        ],\n",
    "                        verbose=1)\n",
    "    \n",
    "    # Log final training loss\n",
    "    mlflow.log_metric(\"final_loss\", float(history.history[\"loss\"][-1]))\n",
    "\n",
    "    # --- Forecasting ---\n",
    "    X_pred_enc = X_enc[-1:]  # Last known sequence for encoder\n",
    "    X_pred_dec = np.zeros((1, SEQ_LEN, 1), dtype=dtype)\n",
    "\n",
    "    # Seed decoder with the last known log1p target value\n",
    "    last_log_target = target_scaler.transform([[np.log1p(df[\"unit_sales\"].iloc[-1])]])[0, 0]\n",
    "    X_pred_dec[:, 0, 0] = last_log_target\n",
    "\n",
    "    # Empty forecast array\n",
    "    forecast_scaled = np.zeros((SEQ_LEN, 1), dtype=dtype)\n",
    "\n",
    "    # Auto-regressive forecasting\n",
    "    for t in range(SEQ_LEN):\n",
    "        pred_step = model.predict([X_pred_enc, X_pred_dec], verbose=0)[0]\n",
    "        forecast_scaled[t, 0] = pred_step[t, 0]\n",
    "        if t + 1 < SEQ_LEN:\n",
    "            X_pred_dec[0, t + 1, 0] = forecast_scaled[t, 0]\n",
    "\n",
    "    # Inverse transform and convert back from log scale\n",
    "    forecast_log = target_scaler.inverse_transform(forecast_scaled)\n",
    "    forecast = np.expm1(forecast_log).flatten()\n",
    "\n",
    "    # --- Plot forecast ---\n",
    "    future_dates = pd.date_range(df[\"date\"].max() + pd.Timedelta(days=1), periods=SEQ_LEN)\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.plot(future_dates, forecast, label=\"Forecast\", linewidth=2, linestyle=\"--\")\n",
    "    plt.title(\"LSTM Seq2Seq Forecast (90 Days)\")\n",
    "    plt.xlabel(\"Date\")\n",
    "    plt.ylabel(\"Unit Sales\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Save and log forecast plot\n",
    "    plot_path = \"forecast_log1p_plot.png\"\n",
    "    plt.savefig(plot_path)\n",
    "    mlflow.log_artifact(plot_path)\n",
    "    plt.close()\n",
    "\n",
    "    # --- Save model and scaler ---\n",
    "    model_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    scaler_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    model.save(model_path)\n",
    "    joblib.dump(target_scaler, scaler_path)\n",
    "\n",
    "    # Log files to MLflow\n",
    "    mlflow.log_artifact(str(model_path))\n",
    "    mlflow.log_artifact(str(scaler_path))\n",
    "\n",
    "    print(\"âœ… Model and forecast successfully saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81987785",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
