{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb4c5c01",
   "metadata": {},
   "source": [
    "# 0. Environment Setup with pip\n",
    "\n",
    "This command installs all required libraries in one step for a macOS-based development environment. It includes deep learning (Keras/TensorFlow), machine learning (XGBoost), hyperparameter optimization (Hyperopt, Optuna), experiment tracking (MLflow), and model wrapping (Scikeras)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5f1cc4a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Ignoring invalid distribution ~ensorflow (/opt/anaconda3/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~ensorflow (/opt/anaconda3/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~ensorflow (/opt/anaconda3/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -q --disable-pip-version-check \\\n",
    "    keras \\\n",
    "    tensorflow-macos \\\n",
    "    tensorflow-metal \\\n",
    "    xgboost \\\n",
    "    hyperopt \\\n",
    "    mlflow \\\n",
    "    optuna \\\n",
    "    scikeras \\\n",
    "    gdown"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e626de0a",
   "metadata": {},
   "source": [
    "# 1. Import libraries\n",
    "\n",
    "This section imports all necessary Python libraries and modules required for data handling, model building, evaluation, optimization, and visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2a0ed144",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard Library\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Data Handling\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Preprocessing & Splitting\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder, RobustScaler\n",
    "\n",
    "# Metrics\n",
    "from sklearn.metrics import (\n",
    "    mean_absolute_error,\n",
    "    mean_squared_error,\n",
    "    r2_score\n",
    ")\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "from scikeras.wrappers import KerasRegressor \n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Dropout\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "# Traditional ML Models\n",
    "import xgboost as xgb\n",
    "\n",
    "# Deep Learning (Keras / TensorFlow)\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, Model, load_model\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks  import EarlyStopping, ReduceLROnPlateau\n",
    "# Hyperparameter Optimization\n",
    "from hyperopt import fmin, tpe, hp, Trials, STATUS_OK\n",
    "\n",
    "# Experiment Tracking with MLflow\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "import mlflow.keras\n",
    "from mlflow.models.signature import infer_signature\n",
    "\n",
    "# Model Persistence\n",
    "import joblib\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a14d61a1",
   "metadata": {},
   "source": [
    "This snippet ensures the script can access project-level configuration regardless of execution context. It dynamically sets the project root, adjusts the system path for imports, and loads configuration constants needed for forecasting workflows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e5c59fc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features: ['store_nbr', 'item_nbr', 'onpromotion', 'day_of_week', 'month', 'unit_sales_7d_avg', 'lag_1', 'lag_7', 'rolling_mean_7']\n",
      "Sequence Length: 90\n",
      "Target: unit_sales\n",
      "Cutoff Date: 2013-12-31 00:00:00\n",
      "Forecast End: 2014-03-31\n",
      "Hyperopt Space: {'max_depth': [3, 4, 5, 6], 'learning_rate_range': (0.01, 0.1, 0.2, 0.3), 'n_estimators': [20, 50, 100]}\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Get project root one level up from current working directory (for Jupyter use)\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "\n",
    "# Add project root to Python path if not already included\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n",
    "\n",
    "# Import constants from Streamlit app configuration\n",
    "import importlib\n",
    "import app.config as cfg\n",
    "importlib.reload(cfg)\n",
    "\n",
    "import model.model_utils as model_utils\n",
    "\n",
    "\n",
    "\n",
    "def main():\n",
    "    print(\"Features:\", cfg.FEATURES)\n",
    "    print(\"Sequence Length:\", cfg.SEQ_LEN)\n",
    "    print(\"Target:\", cfg.TARGET)\n",
    "    print(\"Cutoff Date:\", cfg.CUTOFF_DATE)\n",
    "    print(\"Forecast End:\", cfg.FORECAST_END)\n",
    "    print(\"Hyperopt Space:\", cfg.HYPEROPT_SPACE)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79263f93",
   "metadata": {},
   "source": [
    "# 2. Import Data\n",
    "\n",
    "Import data prepared in the first notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f2a7f30c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(\"../data/preprocessed_data/train_guayas_prepared.csv\", parse_dates=['date'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb82d5b3",
   "metadata": {},
   "source": [
    "# 3. Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08aab94d",
   "metadata": {},
   "source": [
    "## 3.1 Create feature and set dtypes\n",
    "\n",
    "This section performs essential preprocessing tasks, such as datetime conversion, label encoding, memory optimization, and feature engineering. It includes lag features and rolling statistics, which are commonly used in time series forecasting models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bd5da0ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        date  unit_sales  lag_1  lag_7  rolling_mean_7\n",
      "0 2013-01-09         2.0    0.0    0.0        0.000000\n",
      "1 2013-01-10         0.0    2.0    0.0        0.285714\n",
      "2 2013-01-11         0.0    0.0    0.0        0.285714\n",
      "3 2013-01-12         2.0    0.0    0.0        0.285714\n",
      "4 2013-01-13         0.0    2.0    0.0        0.571429\n",
      "5 2013-01-14         0.0    0.0    0.0        0.571429\n",
      "6 2013-01-15         0.0    0.0    0.0        0.571429\n",
      "7 2013-01-16         1.0    0.0    2.0        0.571429\n",
      "8 2013-01-17         2.0    1.0    0.0        0.428571\n",
      "9 2013-01-18         0.0    2.0    0.0        0.714286\n"
     ]
    }
   ],
   "source": [
    "# 1. Convert the 'date' column to datetime\n",
    "df_train['date'] = pd.to_datetime(df_train['date'])\n",
    "\n",
    "# 2. Label-encode family on train, then apply same encoder to test\n",
    "le = LabelEncoder()\n",
    "df_train['family_code'] = le.fit_transform(df_train['family'])\n",
    "df_train ['family_code'] = le.transform   (df_train ['family']).astype('uint8')\n",
    "\n",
    "# 3. Downcast integer columns to reduce memory usage\n",
    "df_train['store_nbr']   = df_train['store_nbr'].astype('uint8')\n",
    "df_train['item_nbr']    = df_train['item_nbr'].astype('uint32')\n",
    "df_train['day_of_week'] = df_train['day_of_week'].astype('uint8')\n",
    "df_train['month']       = df_train['month'].astype('uint8')\n",
    "df_train['year']        = df_train['year'].astype('uint16')\n",
    "df_train['onpromotion'] = df_train['onpromotion'].astype('bool')\n",
    "\n",
    "# 4. Downcast other numeric columns where possible\n",
    "df_train['unit_sales']       = pd.to_numeric(df_train['unit_sales'],       downcast='float')\n",
    "df_train['unit_sales_7d_avg'] = pd.to_numeric(df_train['unit_sales_7d_avg'], downcast='float')\n",
    "\n",
    "# 5. Lag features\n",
    "df_train['lag_1'] = df_train['unit_sales'].shift(1)\n",
    "df_train['lag_7'] = df_train['unit_sales'].shift(7)\n",
    "\n",
    "# 6. Rolling-Window-Features\n",
    "df_train['rolling_mean_7']     = df_train['unit_sales'].shift(1).rolling(window=7).mean()\n",
    "df_train['unit_sales_7d_avg']  = df_train['unit_sales'].shift(1).rolling(window=7).mean()\n",
    "\n",
    "# 7. Drop rows with NaN values\n",
    "df_train = df_train.dropna(subset=['lag_1', 'lag_7', 'rolling_mean_7']).reset_index(drop=True)\n",
    "\n",
    "# 8. Drop unnecessary columns\n",
    "int_cols = df_train.select_dtypes(include='int').columns\n",
    "df_train[int_cols] = df_train[int_cols].astype(\"float16\")\n",
    "\n",
    "print(df_train.filter(['date','unit_sales','lag_1','lag_7','rolling_mean_7']).head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8274c78",
   "metadata": {},
   "source": [
    "## 3.2 Save the dataset\n",
    "\n",
    "After the dataset is prepared and further features were included, the dataset has to be saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a10fa834",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.to_csv(\"../data/preprocessed_data/train_guayas_model_ready.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48c3a2af",
   "metadata": {},
   "source": [
    "## 3.3 Filter for best Store-Item-combination\n",
    "\n",
    "This section filters the dataset to include only data from 2013 and computes basic statistics per store and item combination. It selects only combinations with sufficient data coverage and calculates a custom score based on mean and standard deviation of sales. The best-performing combination is selected to serve as the focus for subsequent model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ec788b07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model for store 24 and item 220435\n"
     ]
    }
   ],
   "source": [
    "# Filter 2013 data only\n",
    "df_2013 = df_train[df_train[\"date\"].dt.year == 2013]\n",
    "\n",
    "# Group by store and item to compute statistics\n",
    "stats = df_2013.groupby([\"store_nbr\", \"item_nbr\"]).agg(\n",
    "    count_days=(\"unit_sales\", \"count\"),\n",
    "    mean_sales=(\"unit_sales\", \"mean\"),\n",
    "    std_sales=(\"unit_sales\", \"std\"),\n",
    "    positive_days=(\"unit_sales\", lambda x: (x > 0).sum())\n",
    ").reset_index()\n",
    "\n",
    "# Filter only combinations with sufficient data\n",
    "stats = stats[stats[\"count_days\"] >= 300]\n",
    "\n",
    "# Compute a custom score: mean / std\n",
    "stats[\"score\"] = stats[\"mean_sales\"] / (stats[\"std_sales\"] + 1e-5)\n",
    "\n",
    "# Get the best combination\n",
    "best_combo = stats.sort_values(\"score\", ascending=False).iloc[0]\n",
    "store_id = int(best_combo[\"store_nbr\"])\n",
    "item_id = int(best_combo[\"item_nbr\"])\n",
    "\n",
    "# Filter data for selected combination\n",
    "print(f\"Training model for store {store_id} and item {item_id}\")\n",
    "df = df_train[(df_train[\"store_nbr\"] == store_id) & (df_train[\"item_nbr\"] == item_id)].copy()\n",
    "df_train = df.sort_values(\"date\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e4cf796",
   "metadata": {},
   "source": [
    "## 3.4 Train/Test-Split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84772adc",
   "metadata": {},
   "source": [
    "This section splits the dataset into training and test sets based on a predefined `CUTOFF_DATE`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a9e9f683",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/Test Split with Cutoff from config\n",
    "train_df = df_train.loc[df_train[\"date\"] <= cfg.CUTOFF_DATE].copy()\n",
    "test_df  = df_train.loc[df_train[\"date\"] >  cfg.CUTOFF_DATE].copy()\n",
    "\n",
    "non_numeric_cols = df_train.select_dtypes(include=[\"datetime64[ns]\", \"datetime64\", \"object\"]).columns\n",
    "\n",
    "X_train = train_df.drop(columns=non_numeric_cols).drop(columns=[cfg.TARGET])\n",
    "y_train = np.log1p(train_df[cfg.TARGET])\n",
    "X_test  = test_df.drop(columns=non_numeric_cols).drop(columns=[cfg.TARGET])\n",
    "y_test  = np.log1p(test_df[cfg.TARGET])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9025b5e",
   "metadata": {},
   "source": [
    "# 4. XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31e852c5",
   "metadata": {},
   "source": [
    "## 4.1 Hyperparameter-Optimized XGBoost Model (Hyperopt)\n",
    "\n",
    "This code performs **automated hyperparameter tuning** and training of an XGBoost model to forecast `unit_sales` for a given store and item using **Hyperopt** and **MLflow**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f77377d",
   "metadata": {},
   "source": [
    "### Purpose\n",
    "To find the best-performing XGBoost model configuration for each store/item combination through hyperparameter optimization. The goal is to minimize forecast error using a data-driven, repeatable search strategy.\n",
    "\n",
    "### What the code does\n",
    "- Defines a **hyperparameter search space** for key XGBoost settings (e.g., `max_depth`, `learning_rate`, `n_estimators`)\n",
    "- Uses **Hyperopt** to explore this space with a loss function based on `MAE`\n",
    "- Trains 25 model variations (`max_evals=25`) on the training set\n",
    "- Selects the best model and evaluates it on the test set\n",
    "- Logs the selected model, parameters, and metrics (`MAE`, `R¬≤`) to **MLflow**\n",
    "- Saves the final model to disk for reuse or deployment\n",
    "\n",
    "This setup supports consistent, scalable model tuning and tracking, enabling per-store/item optimization for time series forecasting tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8aa583ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 25/25 [00:01<00:00, 14.68trial/s, best loss: 8.11827278137207] \n",
      "Best Hyperopt MAE: 8.1183\n",
      "Best Hyperopt R2 Score: -0.0425\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/mlflow/types/utils.py:452: UserWarning: Hint: Inferred schema contains integer column(s). Integer columns in Python cannot represent missing values. If your input data contains missing values at inference time, it will be encoded as floats and will cause a schema enforcement error. The best way to avoid this problem is to infer the model schema based on a realistic data sample (training dataset) that includes missing values. Alternatively, you can declare integer columns as doubles (float64) whenever these columns may have missing values. See `Handling Integers With Missing Values <https://www.mlflow.org/docs/latest/models.html#handling-integers-with-missing-values>`_ for more details.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# --- HYPEROPT XGBOOST TRAINING ---\n",
    "# End any active MLflow run\n",
    "if mlflow.active_run():\n",
    "    mlflow.end_run()\n",
    "\n",
    "mlruns_path = os.path.join(project_root, \"mlruns\", \"xgb_hyperopt\")\n",
    "if not os.path.exists(mlruns_path):\n",
    "    # Create the directory if it doesn't exist      \n",
    "        os.makedirs(mlruns_path, exist_ok=True)\n",
    "\n",
    "# Set MLflow tracking URI to the local directory\n",
    "mlflow.set_tracking_uri(f\"file://{mlruns_path}\")\n",
    "# Set the experiment name\n",
    "mlflow.set_experiment(\"store_item_sales_forecast_hyperopt\")\n",
    "\n",
    "# Define the search space\n",
    "space = {\n",
    "    'max_depth': hp.choice('max_depth', range(3, 10)),\n",
    "    'learning_rate': hp.uniform('learning_rate', 0.01, 0.3),\n",
    "    'n_estimators': hp.choice('n_estimators', range(20, 301, 10)),\n",
    "    'subsample': hp.uniform('subsample', 0.5, 1.0),\n",
    "    'colsample_bytree': hp.uniform('colsample_bytree', 0.5, 1.0)\n",
    "}\n",
    "\n",
    "# Objective function for Hyperopt\n",
    "def objective(params):\n",
    "    model = xgb.XGBRegressor(random_state=42, **params)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = np.expm1(model.predict(X_test))\n",
    "    score = mean_absolute_error(np.expm1(y_test), y_pred)\n",
    "    return {'loss': score, 'status': STATUS_OK, 'model': model}\n",
    "\n",
    "# Run Hyperopt\n",
    "trials = Trials()\n",
    "best = fmin(fn=objective, space=space, algo=tpe.suggest, max_evals=25, trials=trials)\n",
    "\n",
    "# Retrieve best model\n",
    "best_model = trials.best_trial['result']['model']\n",
    "y_pred_log = best_model.predict(X_test)\n",
    "y_pred = np.expm1(y_pred_log)\n",
    "\n",
    "# Evaluate final model\n",
    "mae = mean_absolute_error(np.expm1(y_test), y_pred)\n",
    "r2 = r2_score(np.expm1(y_test), y_pred)\n",
    "with mlflow.start_run():\n",
    "    mlflow.log_param(\"store_id\", store_id)\n",
    "    mlflow.log_param(\"item_id\", item_id)\n",
    "    mlflow.log_params(best)\n",
    "    mlflow.log_metric(\"mae\", mae)\n",
    "    mlflow.log_metric(\"r2_score\", r2)\n",
    "    print(f\"Best Hyperopt MAE: {mae:.4f}\")\n",
    "    print(f\"Best Hyperopt R2 Score: {r2:.4f}\")\n",
    "    signature = infer_signature(X_train, best_model.predict(X_train))\n",
    "    mlflow.sklearn.log_model(\n",
    "        best_model,\n",
    "        \"model\",\n",
    "        signature=signature,\n",
    "        input_example=X_train.head(3)\n",
    "    )\n",
    "\n",
    "    \n",
    "# Finally, persist the trained model\n",
    "os.makedirs(\"../model/xgb/archive/\", exist_ok=True)\n",
    "model_path = f\"../model/xgb/archive/xgb_hyperopt_store{store_id}_item{item_id}.pkl\"\n",
    "joblib.dump(best_model, model_path)\n",
    "mlflow.log_artifact(model_path, artifact_path=\"final_model\")\n",
    "\n",
    "# End any active MLflow run\n",
    "if mlflow.active_run():\n",
    "    mlflow.end_run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ada275b5",
   "metadata": {},
   "source": [
    "and following the plot to visualize the forecast of the XGBoost Hyperopt Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "30dbaa50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_forecast(test_df, y_test, y_pred, store_id, item_id, mae, r2, mlflow_artifact_path):\n",
    "    test_df = test_df.copy()\n",
    "    test_df[\"forecast\"] = y_pred\n",
    "    test_df[\"actual\"] = np.expm1(y_test.values)\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(test_df[\"date\"], test_df[\"actual\"], label=\"Actual\", linewidth=2)\n",
    "    plt.plot(test_df[\"date\"], test_df[\"forecast\"], 'o--', color='red', label=\"Forecast\", linewidth=2)\n",
    "    plt.title(f\"XGBoost Hyperopt Forecast vs. Actual\\nStore {store_id}, Item {item_id}\")\n",
    "    plt.xlabel(\"Date\")\n",
    "    plt.ylabel(\"Unit Sales\")\n",
    "\n",
    "    metrics_text = f\"MAE: {mae:.2f}\\nR¬≤: {r2:.2f}\"\n",
    "    plt.gca().text(0.01, 0.95, metrics_text,\n",
    "                   transform=plt.gca().transAxes,\n",
    "                   fontsize=12,\n",
    "                   verticalalignment='top',\n",
    "                   bbox=dict(boxstyle=\"round\", facecolor=\"#f9f9f9\", alpha=0.8))\n",
    "\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    plot_path = f\"xgb_hyperopt_forecast_plot_store{store_id}_item{item_id}.png\"\n",
    "    plt.savefig(plot_path)\n",
    "    mlflow.log_artifact(plot_path, artifact_path=mlflow_artifact_path)\n",
    "    plt.close()\n",
    "    plt.show()\n",
    "\n",
    "# --- LOAD FINAL MODEL AND PREDICT ---\n",
    "store_id = 24\n",
    "item_id = 220435\n",
    "model_path = f\"../model/xgb/archive/xgb_hyperopt_store{store_id}_item{item_id}.pkl\"\n",
    "\n",
    "# load Model\n",
    "loaded_model = joblib.load(model_path)\n",
    "\n",
    "# create forecast\n",
    "y_pred_log = loaded_model.predict(X_test)\n",
    "y_pred = np.expm1(y_pred_log)  # Annahme: Training war auf log1p-Daten\n",
    "\n",
    "# calculate metrics\n",
    "mae = mean_absolute_error(np.expm1(y_test), y_pred)\n",
    "r2 = r2_score(np.expm1(y_test), y_pred)\n",
    "\n",
    "# Forecast-Plot\n",
    "plot_forecast(\n",
    "    test_df=test_df,  # sollte eine Kopie von X_test mit Datumsspalte sein\n",
    "    y_test=y_test,\n",
    "    y_pred=y_pred,\n",
    "    store_id=store_id,\n",
    "    item_id=item_id,\n",
    "    mae=mae,\n",
    "    r2=r2,\n",
    "    mlflow_artifact_path=\"final_model\"\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01791060",
   "metadata": {},
   "source": [
    "## 4.2 XGBoost Regression Model (Global)\n",
    "\n",
    "The following Code is the XGBoost regression model for predicting unit sales using engineered features, evaluate its performance, and log everything with MLflow (including metrics, model, and visualizations)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8a4c761",
   "metadata": {},
   "source": [
    "### Purpose  \n",
    "The purpose of the following code is to train a **global XGBoost model** that predicts unit sales based on multiple engineered features. It aims to capture general sales patterns across all stores and items and evaluate the model‚Äôs performance using robust metrics. The full pipeline is tracked using **MLflow** for transparency and reproducibility.\n",
    "\n",
    "### What the function does\n",
    "- Loads and filters historical sales data up to a defined `CUTOFF_DATE`\n",
    "- Selects relevant features and applies a log transformation to `unit_sales`\n",
    "- Splits the dataset into training and testing subsets\n",
    "- Trains an XGBoost regression model on the processed data\n",
    "- Predicts test values and applies inverse transformation using `expm1`\n",
    "- Evaluates model performance using `MAE` and `R¬≤`\n",
    "- Logs model parameters, evaluation metrics, and plots using **MLflow**\n",
    "- Saves the trained model and feature metadata to disk\n",
    "- Generates and logs:\n",
    "  - A time series plot of unit sales over time\n",
    "  - A forecast vs. actual plot to visualize prediction accuracy\n",
    "\n",
    "This pipeline supports fast experimentation and ensures results are reproducible, interpretable, and ready for deployment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2a6460f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Global model saved to /Users/jennypetschke/VS_Code_Projects/retail_demand_analysis/model/xgb/archive/xgb_global.pkl\n",
      "üìä MAE: 1.0521 | R2: 0.6061\n"
     ]
    }
   ],
   "source": [
    "if mlflow.active_run():             # if the code interrupted here at the beginnig, pls. try again and go ahead (MLFLOW issue)\n",
    "    mlflow.end_run()                # if there is an active run, end it\n",
    "\n",
    "\n",
    "\n",
    "# Feature columns\n",
    "FEATURE_COLS = [\n",
    "    'store_nbr','item_nbr','onpromotion',\n",
    "    'day_of_week','month','year','unit_sales_7d_avg',\n",
    "    'family_code','lag_1','lag_7','rolling_mean_7'\n",
    "]\n",
    "\n",
    "# Load & filter data\n",
    "df = pd.read_csv(os.path.join(project_root, cfg.PREPARED_DATA_FILE), parse_dates=[\"date\"])\n",
    "df = df[df['unit_sales'] >= 0]\n",
    "df = df[df['date'] <= cfg.CUTOFF_DATE]\n",
    "\n",
    "# Define features and target\n",
    "X = df[FEATURE_COLS].astype(\"float64\")\n",
    "y = np.log1p(df['unit_sales'])  # log1p target\n",
    "\n",
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Train model\n",
    "model = xgb.XGBRegressor(random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict and reverse log\n",
    "y_pred_log = model.predict(X_test)\n",
    "y_pred = np.expm1(y_pred_log)\n",
    "y_true = np.expm1(y_test)\n",
    "\n",
    "# Evaluation metrics\n",
    "mae = mean_absolute_error(y_true, y_pred)\n",
    "r2 = r2_score(y_true, y_pred)\n",
    "\n",
    "# MLflow tracking\n",
    "# 7) Start MLflow tracking\n",
    "mlflow_tracking_dir_xgb_global = os.path.join(project_root, \"mlruns\", \"xgb_global\")\n",
    "if not os.path.exists(mlflow_tracking_dir_xgb_global):\n",
    "    # Create the directory if it doesn't exist\n",
    "    os.makedirs(mlflow_tracking_dir_xgb_global, exist_ok=True)\n",
    "\n",
    "# Set MLflow tracking URI to the local directory\n",
    "mlflow.set_tracking_uri(f\"file://{mlflow_tracking_dir_xgb_global}\")\n",
    "# Set the experiment name\n",
    "mlflow.set_experiment(\"global_xgb_sales_forecast\")\n",
    "\n",
    "with mlflow.start_run():\n",
    "    # Log parameters and metrics\n",
    "    mlflow.log_param(\"model_type\", \"xgb_global\")\n",
    "    mlflow.log_param(\"features\", FEATURE_COLS)\n",
    "    mlflow.log_metric(\"mae\", mae)\n",
    "    mlflow.log_metric(\"r2_score\", r2)\n",
    "\n",
    "    # Save model\n",
    "    out_path = Path(project_root, cfg.XGB_ARCHIVE_DIR) / cfg.GLOBAL_XGB_MODEL\n",
    "    out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    joblib.dump({\"model\": model, \"features\": FEATURE_COLS}, out_path)\n",
    "    mlflow.log_artifact(str(out_path), artifact_path=\"model\")\n",
    "\n",
    "    print(f\"‚úÖ Global model saved to {out_path}\")\n",
    "    print(f\"üìä MAE: {mae:.4f} | R2: {r2:.4f}\")\n",
    "\n",
    "    # üìà Plot 1: Time series of unit_sales\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    df_sorted = df.sort_values(\"date\")\n",
    "    plt.plot(df_sorted[\"date\"], df_sorted[\"unit_sales\"], label=\"unit_sales\", alpha=0.8)\n",
    "    plt.title(\"Unit Sales over Time\")\n",
    "    plt.xlabel(\"Date\")\n",
    "    plt.ylabel(\"Unit Sales\")\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    time_series_path = \"unit_sales_over_time.png\"\n",
    "    plt.savefig(time_series_path)\n",
    "    plt.close()\n",
    "    mlflow.log_artifact(time_series_path)\n",
    "\n",
    "    # üìä Plot 2: Forecast vs actuals (sample)\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    sample_size = min(300, len(y_true))\n",
    "    plt.plot(y_true[:sample_size], label=\"Actual\", linewidth=2)\n",
    "    plt.plot(y_pred[:sample_size], label=\"Forecast\", linestyle=\"--\")\n",
    "    plt.title(\"Forecast vs. Actual (Test Set)\")\n",
    "    plt.xlabel(\"Sample\")\n",
    "    plt.ylabel(\"Unit Sales\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    forecast_path = \"forecast_vs_actual.png\"\n",
    "    plt.savefig(forecast_path)\n",
    "    plt.close()\n",
    "    mlflow.log_artifact(forecast_path)\n",
    "\n",
    "# Ensure clean MLflow exit\n",
    "if mlflow.active_run():\n",
    "    mlflow.end_run()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0bc253b",
   "metadata": {},
   "source": [
    "# 5. LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "346632a7",
   "metadata": {},
   "source": [
    "## 5.1 LSTM Sequence-to-Sequence Forecasting Model (Global)\n",
    "\n",
    "This function trains a **global LSTM Seq2Seq model** for multi-step time series forecasting, such as predicting future `unit_sales`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeec43e6",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Purpose\n",
    "The purpose of the following Code is to learn general sales patterns from historical data and forecast multiple future time steps in a single forward pass. The model is trained on multiple features, like aggregated data, but also on store_nbr and item_nbr.\n",
    "\n",
    "### What the function does\n",
    "- Loads and filters time series data up to a defined `CUTOFF_DATE`\n",
    "- Scales `unit_sales` using MinMaxScaler\n",
    "- Prepares input/output sequences using a sliding window\n",
    "- Builds a Sequence-to-Sequence LSTM model (Encoder‚ÄìDecoder)\n",
    "- Trains the model with callbacks (`EarlyStopping`, `ReduceLROnPlateau`)\n",
    "- Logs all key parameters, metrics, and artifacts using **MLflow**\n",
    "- Saves both the trained model and the scaler for future use\n",
    "\n",
    "This setup ensures that the entire training process is tracked, reproducible, and ready for deployment or experimentation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "19255fa9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# Imports\\nimport os\\nimport joblib\\nimport mlflow\\nimport pandas as pd\\nimport numpy as np\\nimport matplotlib.pyplot as plt\\nfrom pathlib import Path\\nfrom sklearn.metrics import mean_absolute_error, r2_score\\nfrom sklearn.preprocessing import RobustScaler\\nfrom tensorflow.keras.models import Model\\nfrom tensorflow.keras.layers import Input, Dense, LSTM\\nfrom tensorflow.keras.optimizers import Adam\\nfrom tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\\nimport sys\\nimport os\\n\\n# Get project root one level up from current working directory (for Jupyter use)\\nproject_root = os.path.abspath(os.path.join(os.getcwd(), \\'..\\'))\\n\\n# Add project root to Python path if not already included\\nif project_root not in sys.path:\\n    sys.path.insert(0, project_root)\\n\\n# Import constants from Streamlit app configuration\\nimport importlib\\nimport app.config as cfg\\nimportlib.reload(cfg)\\n\\nimport model.model_utils as model_utils\\nimport app.config as cfg\\n\\nseq_len = int(cfg.SEQ_LEN/30)\\n\\n# --- Imports ---\\n# --- Projektstruktur und Pfade ---\\nproject_root = os.path.abspath(os.path.join(os.getcwd(), \\'..\\'))\\ndata_path = os.path.join(project_root, cfg.PREPARED_DATA_FILE)\\n\\n# --- MLflow vorbereiten ---\\nmlflow_tracking_dir = os.path.join(project_root, cfg.MLFLOW_LSTM_SEQ)\\nos.makedirs(mlflow_tracking_dir, exist_ok=True)\\nmlflow.set_tracking_uri(f\"file://{mlflow_tracking_dir}\")\\nmlflow.set_experiment(\"lstm_global_seq2seq_forecast\")\\nif mlflow.active_run():\\n    mlflow.end_run()\\n\\n# --- Daten laden ---\\ndf = pd.read_csv(data_path, parse_dates=[\"date\"])\\ndf = df[df[\\'unit_sales\\'] >= 0].sort_values(\\'date\\').reset_index(drop=True)\\n\\n# --- Skalierung vorbereiten ---\\ntrain_df = df[df[\\'date\\'] <= cfg.CUTOFF_DATE]\\nscaler = RobustScaler()\\nscaler.fit(train_df[[\\'unit_sales\\']])\\ndf[\\'unit_sales_scaled\\'] = scaler.transform(df[[\\'unit_sales\\']])\\n\\n# --- Sequenzen erstellen ---\\ndef make_seq2seq(data, dates, n_in, n_out):\\n    X, y, d = [], [], []\\n    for i in range(len(data) - n_in - n_out + 1):\\n        X.append(data[i:i+n_in, 0])\\n        y.append(data[i+n_in:i+n_in+n_out, 0])\\n        d.append(dates[i+n_in - 1])\\n    return np.array(X), np.array(y), np.array(d)\\n\\nX_all, y_all, date_all = make_seq2seq(\\n    df[[\\'unit_sales_scaled\\']].values,\\n    df[\\'date\\'].values,\\n    cfg.SEQ_LEN, cfg.SEQ_LEN\\n)\\n\\n# --- Train/Test-Split nach Datum ---\\ncutoff = np.datetime64(cfg.CUTOFF_DATE)\\ntrain_mask = date_all <= cutoff\\ntest_mask  = date_all > cutoff\\n\\nX_train, y_train = X_all[train_mask], y_all[train_mask]\\nX_test,  y_test  = X_all[test_mask],  y_all[test_mask]\\n\\n# --- Reshape f√ºr LSTM ---\\ndef reshape_seq(X, y, seq_len):\\n    X_enc = X.reshape(-1, seq_len, 1)\\n    X_dec = np.zeros((len(X), seq_len, 1))\\n    X_dec[:, 1:, :] = y[:, :-1].reshape(len(y), seq_len - 1, 1)\\n    y_seq = y.reshape(len(y), seq_len, 1)\\n    return X_enc, X_dec, y_seq\\n\\nX_enc_train, X_dec_train, y_seq_train = reshape_seq(X_train, y_train, cfg.SEQ_LEN)\\nX_enc_test,  X_dec_test,  y_seq_test  = reshape_seq(X_test,  y_test, cfg.SEQ_LEN)\\n\\n# --- Modell definieren ---\\nenc_inputs = Input(shape=(cfg.SEQ_LEN, 1), name=\"encoder_input\")\\nenc_outputs, state_h, state_c = LSTM(16, return_state=True, name=\"encoder_lstm\")(enc_inputs)\\ndec_inputs = Input(shape=(cfg.SEQ_LEN, 1), name=\"decoder_input\")\\ndec_lstm = LSTM(16, return_sequences=True, name=\"decoder_lstm\")(dec_inputs, initial_state=[state_h, state_c])\\noutputs = Dense(1, activation=\"relu\", name=\"output_dense\")(dec_lstm)\\n\\nmodel = Model([enc_inputs, dec_inputs], outputs)\\nmodel.compile(optimizer=Adam(0.01), loss=\"mae\")\\n\\n# --- Modell√ºbersicht speichern ---\\nsummary_path = os.path.join(project_root, cfg.LSTM_ARCHIVE_DIR, \"model_summary_pretraining.txt\")\\nos.makedirs(os.path.dirname(summary_path), exist_ok=True)\\nwith open(summary_path, \"w\") as f:\\n    model.summary(print_fn=lambda x: f.write(x + \"\\n\"))\\n\\n# --- MLflow Logging ---\\nwith mlflow.start_run():\\n    mlflow.log_params({\\n        \"model_type\": \"LSTM Seq2Seq\",\\n        \"seq_len\": cfg.SEQ_LEN,\\n        \"units\": 16,\\n        \"optimizer\": \"adam\",\\n        \"loss\": \"mae\",\\n        \"activation\": \"relu\",\\n        \"batch_size\": 256,\\n        \"epochs\": 10,\\n        \"cutoff_date\": cfg.CUTOFF_DATE.strftime(\\'%Y-%m-%d\\'),\\n        \"target\": cfg.TARGET,\\n        \"scaler\": \"RobustScaler\"\\n    })\\n\\n    # --- Training ---\\n    early_stop = EarlyStopping(monitor=\"loss\", patience=3, restore_best_weights=True)\\n    reduce_lr = ReduceLROnPlateau(monitor=\"loss\", factor=0.5, patience=2)\\n\\n    history = model.fit(\\n        [X_enc_train, X_dec_train],\\n        y_seq_train,\\n        epochs=10,\\n        batch_size=256,\\n        callbacks=[early_stop, reduce_lr],\\n        verbose=1\\n    )\\n\\n    mlflow.log_metric(\"final_loss\", float(history.history[\"loss\"][-1]))\\n\\n    # --- Rolling Forecast f√ºr 90 Tage in 30er Chunks ---\\n    def rolling_forecast(model, start_sequence, forecast_days, chunk_size):\\n        n_chunks = forecast_days // chunk_size\\n        current_input = start_sequence.reshape(1, chunk_size, 1)\\n        forecast_result = []\\n        for _ in range(n_chunks):\\n            decoder_input = np.zeros((1, chunk_size, 1))\\n            pred = model.predict([current_input, decoder_input], verbose=0)\\n            forecast_result.append(pred.flatten())\\n            current_input = np.roll(current_input, -chunk_size, axis=1)\\n            current_input[0, -chunk_size:, 0] = pred.flatten()\\n        return np.concatenate(forecast_result)\\n\\n    # --- Letzte Sequenz aus Trainingsdaten als Startpunkt ---\\n    last_seq = df[df[\\'date\\'] <= cfg.CUTOFF_DATE][\\'unit_sales_scaled\\'].values[-cfg.SEQ_LEN:]\\n    forecast_scaled = rolling_forecast(model, last_seq, forecast_days=90, chunk_size=cfg.SEQ_LEN)\\n    forecast = scaler.inverse_transform(forecast_scaled.reshape(-1, 1)).flatten()\\n\\n    # --- Plot Forecast ---\\n    plt.figure(figsize=(12, 4))\\n    plt.plot(range(1, 91), forecast, label=\"Forecast (90 Tage)\", linewidth=2)\\n    plt.title(\"90-Tage Rolling LSTM Forecast\")\\n    plt.xlabel(\"Tag\")\\n    plt.ylabel(\"Unit Sales\")\\n    plt.grid(True)\\n    plt.legend()\\n    plt.tight_layout()\\n\\n    plot_path = os.path.join(project_root, \"forecast_rolling_90d.png\")\\n    plt.savefig(plot_path)\\n    mlflow.log_artifact(plot_path)\\n\\n    # --- Modell + Scaler speichern ---\\n    model_path = Path(project_root, cfg.LSTM_ARCHIVE_DIR) / cfg.LSTM_GLOBAL_SEQ2SEQ_MODEL\\n    scaler_path = Path(project_root, cfg.SCALER_ARCHIVE_DIR) / cfg.LSTM_GLOBAL_SEQ2SEQ_SCALER\\n    model.save(model_path)\\n    joblib.dump(scaler, scaler_path)\\n\\n    mlflow.log_artifact(str(model_path), artifact_path=\"model\")\\n    mlflow.log_artifact(str(scaler_path), artifact_path=\"scaler\")\\n    mlflow.log_artifact(summary_path)\\n\\n    print(\"‚úÖ Modell gespeichert.\")\\n    print(f\"   MAE: {mean_absolute_error(y_seq_test.flatten(), model.predict([X_enc_test, X_dec_test]).flatten()):.4f}\")'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''# Imports\n",
    "import os\n",
    "import joblib\n",
    "import mlflow\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, LSTM\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Get project root one level up from current working directory (for Jupyter use)\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "\n",
    "# Add project root to Python path if not already included\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n",
    "\n",
    "# Import constants from Streamlit app configuration\n",
    "import importlib\n",
    "import app.config as cfg\n",
    "importlib.reload(cfg)\n",
    "\n",
    "import model.model_utils as model_utils\n",
    "import app.config as cfg\n",
    "\n",
    "seq_len = int(cfg.SEQ_LEN/30)\n",
    "\n",
    "# --- Imports ---\n",
    "# --- Projektstruktur und Pfade ---\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "data_path = os.path.join(project_root, cfg.PREPARED_DATA_FILE)\n",
    "\n",
    "# --- MLflow vorbereiten ---\n",
    "mlflow_tracking_dir = os.path.join(project_root, cfg.MLFLOW_LSTM_SEQ)\n",
    "os.makedirs(mlflow_tracking_dir, exist_ok=True)\n",
    "mlflow.set_tracking_uri(f\"file://{mlflow_tracking_dir}\")\n",
    "mlflow.set_experiment(\"lstm_global_seq2seq_forecast\")\n",
    "if mlflow.active_run():\n",
    "    mlflow.end_run()\n",
    "\n",
    "# --- Daten laden ---\n",
    "df = pd.read_csv(data_path, parse_dates=[\"date\"])\n",
    "df = df[df['unit_sales'] >= 0].sort_values('date').reset_index(drop=True)\n",
    "\n",
    "# --- Skalierung vorbereiten ---\n",
    "train_df = df[df['date'] <= cfg.CUTOFF_DATE]\n",
    "scaler = RobustScaler()\n",
    "scaler.fit(train_df[['unit_sales']])\n",
    "df['unit_sales_scaled'] = scaler.transform(df[['unit_sales']])\n",
    "\n",
    "# --- Sequenzen erstellen ---\n",
    "def make_seq2seq(data, dates, n_in, n_out):\n",
    "    X, y, d = [], [], []\n",
    "    for i in range(len(data) - n_in - n_out + 1):\n",
    "        X.append(data[i:i+n_in, 0])\n",
    "        y.append(data[i+n_in:i+n_in+n_out, 0])\n",
    "        d.append(dates[i+n_in - 1])\n",
    "    return np.array(X), np.array(y), np.array(d)\n",
    "\n",
    "X_all, y_all, date_all = make_seq2seq(\n",
    "    df[['unit_sales_scaled']].values,\n",
    "    df['date'].values,\n",
    "    cfg.SEQ_LEN, cfg.SEQ_LEN\n",
    ")\n",
    "\n",
    "# --- Train/Test-Split nach Datum ---\n",
    "cutoff = np.datetime64(cfg.CUTOFF_DATE)\n",
    "train_mask = date_all <= cutoff\n",
    "test_mask  = date_all > cutoff\n",
    "\n",
    "X_train, y_train = X_all[train_mask], y_all[train_mask]\n",
    "X_test,  y_test  = X_all[test_mask],  y_all[test_mask]\n",
    "\n",
    "# --- Reshape f√ºr LSTM ---\n",
    "def reshape_seq(X, y, seq_len):\n",
    "    X_enc = X.reshape(-1, seq_len, 1)\n",
    "    X_dec = np.zeros((len(X), seq_len, 1))\n",
    "    X_dec[:, 1:, :] = y[:, :-1].reshape(len(y), seq_len - 1, 1)\n",
    "    y_seq = y.reshape(len(y), seq_len, 1)\n",
    "    return X_enc, X_dec, y_seq\n",
    "\n",
    "X_enc_train, X_dec_train, y_seq_train = reshape_seq(X_train, y_train, cfg.SEQ_LEN)\n",
    "X_enc_test,  X_dec_test,  y_seq_test  = reshape_seq(X_test,  y_test, cfg.SEQ_LEN)\n",
    "\n",
    "# --- Modell definieren ---\n",
    "enc_inputs = Input(shape=(cfg.SEQ_LEN, 1), name=\"encoder_input\")\n",
    "enc_outputs, state_h, state_c = LSTM(16, return_state=True, name=\"encoder_lstm\")(enc_inputs)\n",
    "dec_inputs = Input(shape=(cfg.SEQ_LEN, 1), name=\"decoder_input\")\n",
    "dec_lstm = LSTM(16, return_sequences=True, name=\"decoder_lstm\")(dec_inputs, initial_state=[state_h, state_c])\n",
    "outputs = Dense(1, activation=\"relu\", name=\"output_dense\")(dec_lstm)\n",
    "\n",
    "model = Model([enc_inputs, dec_inputs], outputs)\n",
    "model.compile(optimizer=Adam(0.01), loss=\"mae\")\n",
    "\n",
    "# --- Modell√ºbersicht speichern ---\n",
    "summary_path = os.path.join(project_root, cfg.LSTM_ARCHIVE_DIR, \"model_summary_pretraining.txt\")\n",
    "os.makedirs(os.path.dirname(summary_path), exist_ok=True)\n",
    "with open(summary_path, \"w\") as f:\n",
    "    model.summary(print_fn=lambda x: f.write(x + \"\\n\"))\n",
    "\n",
    "# --- MLflow Logging ---\n",
    "with mlflow.start_run():\n",
    "    mlflow.log_params({\n",
    "        \"model_type\": \"LSTM Seq2Seq\",\n",
    "        \"seq_len\": cfg.SEQ_LEN,\n",
    "        \"units\": 16,\n",
    "        \"optimizer\": \"adam\",\n",
    "        \"loss\": \"mae\",\n",
    "        \"activation\": \"relu\",\n",
    "        \"batch_size\": 256,\n",
    "        \"epochs\": 10,\n",
    "        \"cutoff_date\": cfg.CUTOFF_DATE.strftime('%Y-%m-%d'),\n",
    "        \"target\": cfg.TARGET,\n",
    "        \"scaler\": \"RobustScaler\"\n",
    "    })\n",
    "\n",
    "    # --- Training ---\n",
    "    early_stop = EarlyStopping(monitor=\"loss\", patience=3, restore_best_weights=True)\n",
    "    reduce_lr = ReduceLROnPlateau(monitor=\"loss\", factor=0.5, patience=2)\n",
    "\n",
    "    history = model.fit(\n",
    "        [X_enc_train, X_dec_train],\n",
    "        y_seq_train,\n",
    "        epochs=10,\n",
    "        batch_size=256,\n",
    "        callbacks=[early_stop, reduce_lr],\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    mlflow.log_metric(\"final_loss\", float(history.history[\"loss\"][-1]))\n",
    "\n",
    "    # --- Rolling Forecast f√ºr 90 Tage in 30er Chunks ---\n",
    "    def rolling_forecast(model, start_sequence, forecast_days, chunk_size):\n",
    "        n_chunks = forecast_days // chunk_size\n",
    "        current_input = start_sequence.reshape(1, chunk_size, 1)\n",
    "        forecast_result = []\n",
    "        for _ in range(n_chunks):\n",
    "            decoder_input = np.zeros((1, chunk_size, 1))\n",
    "            pred = model.predict([current_input, decoder_input], verbose=0)\n",
    "            forecast_result.append(pred.flatten())\n",
    "            current_input = np.roll(current_input, -chunk_size, axis=1)\n",
    "            current_input[0, -chunk_size:, 0] = pred.flatten()\n",
    "        return np.concatenate(forecast_result)\n",
    "\n",
    "    # --- Letzte Sequenz aus Trainingsdaten als Startpunkt ---\n",
    "    last_seq = df[df['date'] <= cfg.CUTOFF_DATE]['unit_sales_scaled'].values[-cfg.SEQ_LEN:]\n",
    "    forecast_scaled = rolling_forecast(model, last_seq, forecast_days=90, chunk_size=cfg.SEQ_LEN)\n",
    "    forecast = scaler.inverse_transform(forecast_scaled.reshape(-1, 1)).flatten()\n",
    "\n",
    "    # --- Plot Forecast ---\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    plt.plot(range(1, 91), forecast, label=\"Forecast (90 Tage)\", linewidth=2)\n",
    "    plt.title(\"90-Tage Rolling LSTM Forecast\")\n",
    "    plt.xlabel(\"Tag\")\n",
    "    plt.ylabel(\"Unit Sales\")\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "\n",
    "    plot_path = os.path.join(project_root, \"forecast_rolling_90d.png\")\n",
    "    plt.savefig(plot_path)\n",
    "    mlflow.log_artifact(plot_path)\n",
    "\n",
    "    # --- Modell + Scaler speichern ---\n",
    "    model_path = Path(project_root, cfg.LSTM_ARCHIVE_DIR) / cfg.LSTM_GLOBAL_SEQ2SEQ_MODEL\n",
    "    scaler_path = Path(project_root, cfg.SCALER_ARCHIVE_DIR) / cfg.LSTM_GLOBAL_SEQ2SEQ_SCALER\n",
    "    model.save(model_path)\n",
    "    joblib.dump(scaler, scaler_path)\n",
    "\n",
    "    mlflow.log_artifact(str(model_path), artifact_path=\"model\")\n",
    "    mlflow.log_artifact(str(scaler_path), artifact_path=\"scaler\")\n",
    "    mlflow.log_artifact(summary_path)\n",
    "\n",
    "    print(\"‚úÖ Modell gespeichert.\")\n",
    "    print(f\"   MAE: {mean_absolute_error(y_seq_test.flatten(), model.predict([X_enc_test, X_dec_test]).flatten()):.4f}\")'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7fe317b4",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_features' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 104\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;124;03m'''import os\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;124;03mimport numpy as np\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03mimport pandas as pd\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;124;03mforecast = scaler.inverse_transform(final_pred_scaled)[:, 0]\u001b[39;00m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;66;03m# --- Vorhersage auf letzte Sequenz ---\u001b[39;00m\n\u001b[0;32m--> 104\u001b[0m X_pred_enc \u001b[38;5;241m=\u001b[39m X_features[\u001b[38;5;241m-\u001b[39mSEQ_LEN:]\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m1\u001b[39m, SEQ_LEN, \u001b[38;5;28mlen\u001b[39m(FEATURES))\n\u001b[1;32m    105\u001b[0m X_pred_dec \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros((\u001b[38;5;241m1\u001b[39m, SEQ_LEN, \u001b[38;5;241m1\u001b[39m), dtype\u001b[38;5;241m=\u001b[39mDTYPE)\n\u001b[1;32m    106\u001b[0m X_pred_dec[:, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m scaled_target[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m]  \u001b[38;5;66;03m# üëà Seed den ersten Decoder-Schritt mit letztem bekannten Target\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X_features' is not defined"
     ]
    }
   ],
   "source": [
    "'''import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "from pathlib import Path\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import matplotlib.pyplot as plt\n",
    "import app.config as cfg\n",
    "\n",
    "# --- Parameter ---\n",
    "SEQ_LEN = 90\n",
    "LSTM_UNITS = 8\n",
    "EPOCHS = 10\n",
    "BATCH_SIZE = 16\n",
    "MAX_SAMPLES = 800\n",
    "DTYPE = \"float16\"\n",
    "FEATURES = ['unit_sales', 'onpromotion', 'lag_1']  # exakt 3 Features\n",
    "\n",
    "# --- Daten laden ---\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "data_path = os.path.join(project_root, cfg.PREPARED_DATA_FILE)\n",
    "df = pd.read_csv(data_path, usecols=FEATURES + ['date'], parse_dates=[\"date\"])\n",
    "df = df[(df[\"unit_sales\"] >= 0) & (df[\"date\"] <= cfg.CUTOFF_DATE)].sort_values(\"date\").reset_index(drop=True)\n",
    "\n",
    "# --- Encoder-Daten skalieren (nur unit_sales!)\n",
    "scaler = MinMaxScaler()\n",
    "scaled_target = scaler.fit_transform(df[['unit_sales']]).astype(DTYPE)\n",
    "\n",
    "# --- Full Feature Matrix f√ºr den Encoder ---\n",
    "X_features = df[FEATURES].astype(DTYPE).values\n",
    "\n",
    "# --- Sequenzdatengenerator ---\n",
    "def make_seq2seq(X_feat, y_target, n_in, n_out):\n",
    "    X_enc, X_dec_in, y_out = [], [], []\n",
    "    for i in range(len(X_feat) - n_in - n_out + 1):\n",
    "        # Encoder input mit allen Features\n",
    "        X_enc.append(X_feat[i : i + n_in])\n",
    "        # Decoder input: nur scaled target\n",
    "        dec_seq = np.zeros((n_out, 1), dtype=DTYPE)\n",
    "        dec_seq[1:, 0] = y_target[i + n_in : i + n_in + n_out - 1, 0]\n",
    "\n",
    "        X_dec_in.append(dec_seq)\n",
    "        # Decoder output: nur target\n",
    "        y_out.append(y_target[i + n_in : i + n_in + n_out])\n",
    "    return np.array(X_enc), np.array(X_dec_in), np.array(y_out)\n",
    "\n",
    "X_enc, X_dec, y_seq = make_seq2seq(X_features, scaled_target, SEQ_LEN, SEQ_LEN)\n",
    "\n",
    "# --- Trimmen f√ºr Speicherbegrenzung ---\n",
    "if len(X_enc) > MAX_SAMPLES:\n",
    "    X_enc = X_enc[-MAX_SAMPLES:]\n",
    "    X_dec = X_dec[-MAX_SAMPLES:]\n",
    "    y_seq = y_seq[-MAX_SAMPLES:]\n",
    "\n",
    "# --- Modell erstellen ---\n",
    "enc_inputs = Input(shape=(SEQ_LEN, len(FEATURES)), name=\"encoder_input\")\n",
    "enc_outputs, state_h, state_c = LSTM(LSTM_UNITS, return_state=True)(enc_inputs)\n",
    "\n",
    "dec_inputs = Input(shape=(SEQ_LEN, 1), name=\"decoder_input\")\n",
    "dec_lstm = LSTM(LSTM_UNITS, return_sequences=True)\n",
    "dec_outputs = dec_lstm(dec_inputs, initial_state=[state_h, state_c])\n",
    "outputs = Dense(1, activation=\"relu\")(dec_outputs)\n",
    "\n",
    "model = Model([enc_inputs, dec_inputs], outputs)\n",
    "model.compile(optimizer=Adam(learning_rate=0.01), loss=\"mae\")\n",
    "\n",
    "# --- Trainieren ---\n",
    "early_stop = EarlyStopping(monitor=\"loss\", patience=3, restore_best_weights=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor=\"loss\", factor=0.5, patience=2)\n",
    "\n",
    "history = model.fit(\n",
    "    [X_enc, X_dec], y_seq,\n",
    "    epochs=EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    callbacks=[early_stop, reduce_lr],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# --- Vorhersage auf letzte Sequenz ---\n",
    "# --- Vorhersage: autoregressiv ---\n",
    "X_pred_enc = X_features[-SEQ_LEN:].reshape(1, SEQ_LEN, len(FEATURES))\n",
    "\n",
    "# Initialisiere Decoder-Input mit 0\n",
    "decoder_input = np.zeros((1, SEQ_LEN, 1), dtype=DTYPE)\n",
    "\n",
    "# Starte mit letztem bekannten echten Wert (skaliert)\n",
    "decoder_input[0, 0, 0] = scaled_target[-1, 0]\n",
    "\n",
    "# Schrittweises Vorhersagen\n",
    "for t in range(1, SEQ_LEN):\n",
    "    pred_scaled = model.predict([X_pred_enc, decoder_input])[0]  # (1, SEQ_LEN, 1)\n",
    "    decoder_input[:, t, 0] = pred_scaled[t - 1, 0]  # Autoregressiv: vorhergesagter Wert an n√§chste Stelle\n",
    "\n",
    "# Finales Vorhersageergebnis\n",
    "final_pred_scaled = model.predict([X_pred_enc, decoder_input])[0]\n",
    "forecast = scaler.inverse_transform(final_pred_scaled)[:, 0]\n",
    "'''\n",
    "# --- Vorhersage auf letzte Sequenz ---\n",
    "X_pred_enc = X_features[-SEQ_LEN:].reshape(1, SEQ_LEN, len(FEATURES))\n",
    "X_pred_dec = np.zeros((1, SEQ_LEN, 1), dtype=DTYPE)\n",
    "X_pred_dec[:, 0, 0] = scaled_target[-1, 0]  # üëà Seed den ersten Decoder-Schritt mit letztem bekannten Target\n",
    "\n",
    "# Vorhersage (skaliert)\n",
    "pred_scaled = model.predict([X_pred_enc, X_pred_dec])[0]\n",
    "forecast = scaler.inverse_transform(pred_scaled)[:, 0]\n",
    "'''\n",
    "\n",
    "# --- Forecast-Daten ---\n",
    "future_dates = pd.date_range(df[\"date\"].max() + pd.Timedelta(days=1), periods=SEQ_LEN)\n",
    "actuals_df = df.groupby(\"date\", as_index=True)[\"unit_sales\"].mean()\n",
    "actuals = actuals_df.reindex(future_dates).values\n",
    "\n",
    "\n",
    "# --- Plot ---\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(df[\"date\"][-SEQ_LEN:], df[\"unit_sales\"][-SEQ_LEN:], label=\"Last Seen\", linewidth=2)\n",
    "plt.plot(future_dates, forecast, \"r--\", label=\"Forecast\", linewidth=2)\n",
    "plt.title(\"LSTM Seq2Seq Forecast (90 Days)\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Unit Sales\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# --- Modell speichern ---\n",
    "model_path = Path(project_root) / cfg.LSTM_ARCHIVE_DIR / cfg.LSTM_GLOBAL_MODEL\n",
    "scaler_path = Path(project_root) / cfg.SCALER_ARCHIVE_DIR / cfg.LSTM_GLOBAL_SCALER\n",
    "model_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "scaler_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "model.save(model_path)\n",
    "joblib.dump(scaler, scaler_path)\n",
    "\n",
    "print(\"‚úÖ Model und Scaler gespeichert.\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "87cc9be9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-27 15:26:22.080348: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:961] PluggableGraphOptimizer failed: INVALID_ARGUMENT: Failed to deserialize the `graph_buf`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m50/50\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 1.0905e-07 - learning_rate: 0.1000\n",
      "Epoch 2/100\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.0000e+00 - learning_rate: 0.1000\n",
      "Epoch 3/100\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 0.0000e+00 - learning_rate: 0.1000\n",
      "Epoch 4/100\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.0000e+00 - learning_rate: 0.0500\n",
      "Epoch 5/100\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.0000e+00 - learning_rate: 0.0500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 157ms/step\n",
      "‚úÖ LSTM (SEQ_LEN=90) gespeichert & in MLflow geloggt.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import mlflow\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import importlib\n",
    "import app.config as cfg\n",
    "importlib.reload(cfg)\n",
    "\n",
    "\n",
    "# --- Parameter ---\n",
    "SEQ_LEN = 90\n",
    "LSTM_UNITS = 8\n",
    "learning_rate = 0.001\n",
    "EPOCHS = 100\n",
    "BATCH_SIZE = 16  # reduziert wegen langer Sequenz\n",
    "MAX_SAMPLES = 800  # Limit der Trainingssequenzen f√ºr Performance\n",
    "DTYPE = \"float16\"\n",
    "FEATURES = ['unit_sales', 'onpromotion', 'lag_1'\n",
    "            ]#, 'rolling_mean_7', 'store_nbr', 'item_nbr'\n",
    "\n",
    "# --- Pfade ---\n",
    "import app.config as cfg\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "data_path = os.path.join(project_root, cfg.PREPARED_DATA_FILE)\n",
    "mlflow_tracking_dir_lstm = os.path.join(project_root, \"mlruns/lstm_global\")\n",
    "\n",
    "# --- Daten laden ---\n",
    "df = pd.read_csv(data_path, parse_dates=[\"date\"])\n",
    "use_cols = FEATURES + ['date']  # 'date' wird zum Parsen ben√∂tigt\n",
    "df = pd.read_csv(data_path, usecols=use_cols, parse_dates=[\"date\"])\n",
    "df = df[(df[\"unit_sales\"] >= 0) & (df[\"date\"] <= cfg.CUTOFF_DATE)].sort_values(\"date\").reset_index(drop=True)\n",
    "\n",
    "# --- Skalierung ---\n",
    "scaler = MinMaxScaler()\n",
    "scaled = scaler.fit_transform(df[FEATURES]).astype(dtype=DTYPE)\n",
    "\n",
    "# --- Sequenzdaten erzeugen ---\n",
    "def make_seq2seq(data, n_in, n_out):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - n_in - n_out + 1):\n",
    "        X.append(data[i : i + n_in, 0])\n",
    "        y.append(data[i + n_in : i + n_in + n_out, 0])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "X_raw, y_raw = make_seq2seq(scaled, SEQ_LEN, SEQ_LEN)\n",
    "n_features = len(FEATURES)\n",
    "# --- Datenformate ---\n",
    "X_enc = X_raw.reshape(-1, SEQ_LEN, n_features).astype(dtype=DTYPE)\n",
    "X_dec = np.zeros((len(X_raw), SEQ_LEN, n_features), dtype=DTYPE)\n",
    "X_dec[:, 1:, :] = y_raw[:, :-1].reshape(len(y_raw), SEQ_LEN - 1, 1)\n",
    "y_seq = y_raw.reshape(len(y_raw), SEQ_LEN, 1).astype(dtype=DTYPE)\n",
    "\n",
    "# --- Begrenze Datensatzgr√∂√üe ---\n",
    "if len(X_enc) > MAX_SAMPLES:\n",
    "    X_enc = X_enc[-MAX_SAMPLES:]\n",
    "    X_dec = X_dec[-MAX_SAMPLES:]\n",
    "    y_seq = y_seq[-MAX_SAMPLES:]\n",
    "\n",
    "# --- LSTM-Modell ---\n",
    "\n",
    "\n",
    "enc_inputs = Input(shape=(SEQ_LEN, n_features), name=\"encoder_input\")\n",
    "enc_outputs, state_h, state_c = LSTM(LSTM_UNITS, return_state=True, name=\"encoder_lstm\")(enc_inputs)\n",
    "dec_inputs = Input(shape=(SEQ_LEN, n_features), name=\"decoder_input\")\n",
    "dec_outputs = LSTM(LSTM_UNITS, return_sequences=True, name=\"decoder_lstm\")(dec_inputs, initial_state=[state_h, state_c])\n",
    "outputs = Dense(1, activation=\"relu\")(dec_outputs)\n",
    "\n",
    "model = Model([enc_inputs, dec_inputs], outputs)\n",
    "model.compile(optimizer=Adam(0.1), loss=\"mae\")\n",
    "\n",
    "# --- MLflow ---\n",
    "mlflow.set_tracking_uri(f\"file://{mlflow_tracking_dir_lstm}\")\n",
    "mlflow.set_experiment(\"global_lstm_seq2seq_forecast\")\n",
    "\n",
    "if mlflow.active_run():\n",
    "    mlflow.end_run()\n",
    "\n",
    "with mlflow.start_run():\n",
    "    mlflow.log_params({\n",
    "        \"model_type\": \"lstm_seq2seq\",\n",
    "        \"seq_len\": SEQ_LEN,\n",
    "        \"lstm_units\": LSTM_UNITS,\n",
    "        \"epochs\": EPOCHS,\n",
    "        \"batch_size\": BATCH_SIZE,\n",
    "        \"sample_size\": len(X_enc)\n",
    "    })\n",
    "\n",
    "    # --- Training ---\n",
    "    early_stop = EarlyStopping(monitor=\"loss\", patience=3, restore_best_weights=True)\n",
    "    reduce_lr = ReduceLROnPlateau(monitor=\"loss\", factor=0.5, patience=2)\n",
    "    history = model.fit(\n",
    "        [X_enc, X_dec], y_seq,\n",
    "        epochs=EPOCHS,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        callbacks=[early_stop, reduce_lr],\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    final_loss = history.history[\"loss\"][-1]\n",
    "    mlflow.log_metric(\"final_loss\", final_loss)\n",
    "\n",
    "    # --- Vorhersage auf letzte Sequenz ---\n",
    "    X_pred_enc = scaled[-SEQ_LEN:].reshape(1, SEQ_LEN, n_features)\n",
    "    X_pred_dec = np.zeros((1, SEQ_LEN, n_features), dtype=\"float32\")\n",
    "    pred_scaled = model.predict([X_pred_enc, X_pred_dec])[0]\n",
    "    # Dummy mit Nullen ‚Äì exakt gleiche Struktur wie beim Scaling (3 Features)\n",
    "    dummy = np.zeros((cfg.SEQ_LEN, len(FEATURES)), dtype=DTYPE)\n",
    "    # Nur unit_sales (Index 0) mit den Vorhersagewerten bef√ºllen\n",
    "    dummy[:, 0] = pred_scaled[:, 0]\n",
    "    # R√ºcktransformieren und nur unit_sales extrahieren\n",
    "    forecast = scaler.inverse_transform(dummy)[:, 0]\n",
    "\n",
    "\n",
    "\n",
    "    # --- Plot ---\n",
    "    future_dates = pd.date_range(df[\"date\"].max() + pd.Timedelta(days=1), periods=SEQ_LEN)\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.plot(df[\"date\"][-SEQ_LEN:], df[\"unit_sales\"][-SEQ_LEN:], label=\"Last Seen\", linewidth=2)\n",
    "    plt.plot(future_dates, forecast, \"r--\", label=\"Forecast\", linewidth=2)\n",
    "    plt.title(\"LSTM Seq2Seq Forecast (90 Days)\")\n",
    "    plt.xlabel(\"Date\")\n",
    "    plt.ylabel(\"Unit Sales\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    plot_path = \"lstm_forecast_plot.png\"\n",
    "    plt.savefig(plot_path)\n",
    "    mlflow.log_artifact(plot_path)\n",
    "    plt.close()\n",
    "\n",
    "    # --- Modell speichern ---\n",
    "    project_root = Path(os.path.abspath(os.path.join(os.getcwd(), '..')))\n",
    "    model_path = Path(project_root) / cfg.LSTM_ARCHIVE_DIR / cfg.LSTM_GLOBAL_MODEL\n",
    "    scaler_path = Path(project_root) / cfg.SCALER_ARCHIVE_DIR / cfg.LSTM_GLOBAL_SCALER\n",
    "\n",
    "\n",
    "    # Ordner anlegen\n",
    "    model_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    scaler_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "    assert str(model_path).endswith(\".keras\"), f\"‚ùå Model path must end with .keras, got {model_path}\"\n",
    "\n",
    "    # Jetzt funktioniert das:\n",
    "    model.save(model_path)\n",
    "    joblib.dump(scaler, scaler_path)\n",
    "\n",
    "    # MLflow\n",
    "    mlflow.log_artifact(str(model_path))\n",
    "    mlflow.log_artifact(str(scaler_path))\n",
    "\n",
    "    print(f\"‚úÖ LSTM (SEQ_LEN={SEQ_LEN}) gespeichert & in MLflow geloggt.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e8537ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-27 13:41:29.911866: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:961] PluggableGraphOptimizer failed: INVALID_ARGUMENT: Failed to deserialize the `graph_buf`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m50/50\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 25ms/step - loss: 7.7279e-07 - learning_rate: 0.1000\n",
      "Epoch 2/10\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - loss: 0.0000e+00 - learning_rate: 0.1000\n",
      "Epoch 3/10\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 50ms/step - loss: 0.0000e+00 - learning_rate: 0.1000\n",
      "Epoch 4/10\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 34ms/step - loss: 0.0000e+00 - learning_rate: 0.0500\n",
      "Epoch 5/10\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - loss: 0.0000e+00 - learning_rate: 0.0500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 625ms/step\n",
      "‚úÖ LSTM (SEQ_LEN=90) gespeichert & in MLflow geloggt.\n"
     ]
    }
   ],
   "source": [
    "# -- NEW --\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import mlflow\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import importlib\n",
    "import app.config as cfg\n",
    "importlib.reload(cfg)\n",
    "\n",
    "\n",
    "# --- Parameter ---\n",
    "SEQ_LEN = 90\n",
    "LSTM_UNITS = 8\n",
    "learning_rate = 0.1\n",
    "EPOCHS = 10\n",
    "BATCH_SIZE = 16  # reduziert wegen langer Sequenz\n",
    "MAX_SAMPLES = 800  # Limit der Trainingssequenzen f√ºr Performance\n",
    "dtype = \"float16\"  # f√ºr geringeren Speicherverbrauch\n",
    "\n",
    "FEATURES = ['unit_sales', 'lag_1', 'rolling_mean_7'\n",
    "            # ,'store_nbr', 'item_nbr', 'onpromotion', \n",
    "            ]\n",
    "\n",
    "# --- Pfade ---\n",
    "import app.config as cfg\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "data_path = os.path.join(project_root, cfg.PREPARED_DATA_FILE)\n",
    "mlflow_tracking_dir_lstm = os.path.join(project_root, \"mlruns/lstm_global\")\n",
    "\n",
    "# --- Daten laden ---\n",
    "df = pd.read_csv(data_path, parse_dates=[\"date\"])\n",
    "use_cols = FEATURES + ['date']  # 'date' wird zum Parsen ben√∂tigt\n",
    "df = pd.read_csv(data_path, usecols=use_cols, parse_dates=[\"date\"])\n",
    "df = df[(df[\"unit_sales\"] >= 0) & (df[\"date\"] <= cfg.CUTOFF_DATE)].sort_values(\"date\").reset_index(drop=True)\n",
    "\n",
    "# --- Skalierung ---\n",
    "scaler = MinMaxScaler()\n",
    "scaled = scaler.fit_transform(df[FEATURES]).astype(dtype)\n",
    "\n",
    "# --- Sequenzdaten erzeugen ---\n",
    "def make_seq2seq(data, n_in, n_out):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - n_in - n_out + 1):\n",
    "        X.append(data[i : i + n_in, :])                # alle Features\n",
    "        y.append(data[i + n_in : i + n_in + n_out, 0]) # nur Target (unit_sales)\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "\n",
    "X_raw, y_raw = make_seq2seq(scaled, SEQ_LEN, SEQ_LEN)\n",
    "\n",
    "X_enc = X_raw.astype(dtype)  # shape: (samples, SEQ_LEN, n_features)\n",
    "X_dec = np.zeros_like(X_enc)     # shape: wie X_enc, wird sp√§ter f√ºr Teacher Forcing genutzt\n",
    "X_dec[:, 1:, :] = X_enc[:, :-1, :]  # nur wenn du ein autoregressives Decoder-Setup m√∂chtest\n",
    "\n",
    "y_seq = y_raw.reshape(len(y_raw), SEQ_LEN, 1).astype(dtype)\n",
    "\n",
    "\n",
    "# --- Begrenze Datensatzgr√∂√üe ---\n",
    "if len(X_enc) > MAX_SAMPLES:\n",
    "    X_enc = X_enc[-MAX_SAMPLES:]\n",
    "    X_dec = X_dec[-MAX_SAMPLES:]\n",
    "    y_seq = y_seq[-MAX_SAMPLES:]\n",
    "\n",
    "# --- LSTM-Modell ---\n",
    "n_features = len(FEATURES)\n",
    "\n",
    "enc_inputs = Input(shape=(SEQ_LEN, n_features), name=\"encoder_input\")\n",
    "enc_outputs, state_h, state_c = LSTM(LSTM_UNITS, return_state=True, name=\"encoder_lstm\")(enc_inputs)\n",
    "dec_inputs = Input(shape=(SEQ_LEN, n_features), name=\"decoder_input\")\n",
    "dec_outputs = LSTM(LSTM_UNITS, return_sequences=True, name=\"decoder_lstm\")(dec_inputs, initial_state=[state_h, state_c])\n",
    "outputs = Dense(1, activation=\"relu\")(dec_outputs)\n",
    "\n",
    "model = Model([enc_inputs, dec_inputs], outputs)\n",
    "model.compile(optimizer=Adam(0.1), loss=\"mae\")\n",
    "\n",
    "# --- MLflow ---\n",
    "mlflow.set_tracking_uri(f\"file://{mlflow_tracking_dir_lstm}\")\n",
    "mlflow.set_experiment(\"global_lstm_seq2seq_forecast\")\n",
    "\n",
    "if mlflow.active_run():\n",
    "    mlflow.end_run()\n",
    "\n",
    "with mlflow.start_run():\n",
    "    mlflow.log_params({\n",
    "        \"model_type\": \"lstm_seq2seq\",\n",
    "        \"seq_len\": SEQ_LEN,\n",
    "        \"lstm_units\": LSTM_UNITS,\n",
    "        \"epochs\": EPOCHS,\n",
    "        \"batch_size\": BATCH_SIZE,\n",
    "        \"sample_size\": len(X_enc)\n",
    "    })\n",
    "\n",
    "    # --- Training ---\n",
    "    early_stop = EarlyStopping(monitor=\"loss\", patience=3, restore_best_weights=True)\n",
    "    reduce_lr = ReduceLROnPlateau(monitor=\"loss\", factor=0.5, patience=2)\n",
    "    history = model.fit(\n",
    "        [X_enc, X_dec], y_seq,\n",
    "        epochs=EPOCHS,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        callbacks=[early_stop, reduce_lr],\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    final_loss = history.history[\"loss\"][-1]\n",
    "    mlflow.log_metric(\"final_loss\", final_loss)\n",
    "\n",
    "    # --- Vorhersage auf letzte Sequenz ---\n",
    "    X_pred_enc = scaled[-SEQ_LEN:].reshape(1, SEQ_LEN, n_features)\n",
    "    X_pred_dec = np.zeros((1, SEQ_LEN, n_features), dtype=dtype)\n",
    "    X_pred_dec[:, 0, :] = X_pred_enc[:, -1, :]  # üëà Seed initialisieren mit letztem echten Schritt\n",
    "\n",
    "\n",
    "    # Vorhersage (skaliert)\n",
    "    pred_scaled = model.predict([X_pred_enc, X_pred_dec])[0]  # shape = (90, 1)\n",
    "\n",
    "    # Dummy-Matrix bauen: alle anderen Features = 0, unit_sales = pred_scaled\n",
    "    dummy = np.zeros((SEQ_LEN, n_features), dtype=dtype)\n",
    "    dummy[:, 0] = pred_scaled[:, 0]  # Annahme: unit_sales ist an Index 0\n",
    "\n",
    "    # R√ºckskalierung: nur unit_sales extrahieren\n",
    "    forecast = scaler.inverse_transform(dummy)[:, 0]\n",
    "\n",
    "\n",
    "    # --- Plot ---\n",
    "    future_dates = pd.date_range(df[\"date\"].max() + pd.Timedelta(days=1), periods=SEQ_LEN)\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.plot(df[\"date\"][-SEQ_LEN:], df[\"unit_sales\"][-SEQ_LEN:], label=\"Last Seen\", linewidth=2)\n",
    "    plt.plot(future_dates, forecast, \"r--\", label=\"Forecast\", linewidth=2)\n",
    "    plt.title(\"LSTM Seq2Seq Forecast (90 Days)\")\n",
    "    plt.xlabel(\"Date\")\n",
    "    plt.ylabel(\"Unit Sales\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    plot_path = \"lstm_forecast_plot.png\"\n",
    "    plt.savefig(plot_path)\n",
    "    mlflow.log_artifact(plot_path)\n",
    "    plt.close()\n",
    "\n",
    "    # --- Modell speichern ---\n",
    "    project_root = Path(os.path.abspath(os.path.join(os.getcwd(), '..')))\n",
    "    model_path = Path(project_root) / cfg.LSTM_ARCHIVE_DIR / cfg.LSTM_GLOBAL_MODEL\n",
    "    scaler_path = Path(project_root) / cfg.SCALER_ARCHIVE_DIR / cfg.LSTM_GLOBAL_SCALER\n",
    "\n",
    "\n",
    "    # Ordner anlegen\n",
    "    model_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    scaler_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "    assert str(model_path).endswith(\".keras\"), f\"‚ùå Model path must end with .keras, got {model_path}\"\n",
    "\n",
    "    # Jetzt funktioniert das:\n",
    "    model.save(model_path)\n",
    "    joblib.dump(scaler, scaler_path)\n",
    "\n",
    "    # MLflow\n",
    "    mlflow.log_artifact(str(model_path))\n",
    "    mlflow.log_artifact(str(scaler_path))\n",
    "\n",
    "    print(f\"‚úÖ LSTM (SEQ_LEN={SEQ_LEN}) gespeichert & in MLflow geloggt.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd89089b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Imports ---\n",
    "import os\n",
    "import sys\n",
    "import importlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import mlflow\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "# --- Config laden ---\n",
    "import app.config as cfg\n",
    "importlib.reload(cfg)\n",
    "\n",
    "# --- Parameter ---\n",
    "SEQ_LEN = cfg.SEQ_LEN\n",
    "LSTM_UNITS = 8\n",
    "EPOCHS = 20\n",
    "BATCH_SIZE = 16\n",
    "MAX_SAMPLES = 800\n",
    "\n",
    "FEATURES = ['unit_sales', 'onpromotion', 'lag_1', 'rolling_mean_7', 'store_nbr', 'item_nbr']\n",
    "\n",
    "# --- Pfade ---\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "data_path = os.path.join(project_root, cfg.PREPARED_DATA_FILE)\n",
    "mlflow_tracking_dir = os.path.join(project_root, cfg.MLFLOW_LSTM_SEQ)\n",
    "model_path = Path(project_root) / cfg.LSTM_ARCHIVE_DIR / cfg.LSTM_GLOBAL_MODEL\n",
    "scaler_path = Path(project_root) / cfg.SCALER_ARCHIVE_DIR / cfg.LSTM_GLOBAL_SCALER\n",
    "\n",
    "# --- Daten laden ---\n",
    "df = pd.read_csv(data_path, usecols=FEATURES + ['date'], parse_dates=['date'])\n",
    "df = df[(df[\"unit_sales\"] >= 0) & (df[\"date\"] <= cfg.CUTOFF_DATE)].sort_values(\"date\").reset_index(drop=True)\n",
    "\n",
    "# --- Target-Transformation (optional log1p) ---\n",
    "df[\"target\"] = np.log1p(df[\"unit_sales\"])  # verwendet log(1 + x) gegen Null-Inflation\n",
    "\n",
    "# --- Skalierung ---\n",
    "scalers = {f: MinMaxScaler() for f in FEATURES}\n",
    "scaled_features = np.stack([scalers[f].fit_transform(df[[f]]).flatten() for f in FEATURES], axis=-1).astype(\"float32\")\n",
    "target_scaler = MinMaxScaler()\n",
    "scaled_target = target_scaler.fit_transform(df[[\"target\"]]).astype(\"float32\")\n",
    "\n",
    "# --- Sequenzen erstellen ---\n",
    "def make_seq2seq(X_data, y_data, n_in, n_out):\n",
    "    X, y = [], []\n",
    "    for i in range(len(X_data) - n_in - n_out + 1):\n",
    "        X.append(X_data[i : i + n_in])\n",
    "        y.append(y_data[i + n_in : i + n_in + n_out, 0])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "X_raw, y_raw = make_seq2seq(scaled_features, scaled_target, SEQ_LEN, SEQ_LEN)\n",
    "\n",
    "# --- Reshape f√ºr LSTM ---\n",
    "X_enc = X_raw.astype(\"float32\")\n",
    "X_dec = np.zeros((len(X_raw), SEQ_LEN, 1), dtype=\"float32\")\n",
    "X_dec[:, 1:, :] = y_raw[:, :-1].reshape(len(y_raw), SEQ_LEN - 1, 1)\n",
    "X_dec[:, 0, 0] = X_enc[:, -1, 0]  # letzte bekannte Info als Startwert\n",
    "y_seq = y_raw.reshape(len(y_raw), SEQ_LEN, 1).astype(\"float32\")\n",
    "\n",
    "# --- Sample-Gr√∂√üe begrenzen ---\n",
    "if len(X_enc) > MAX_SAMPLES:\n",
    "    X_enc = X_enc[-MAX_SAMPLES:]\n",
    "    X_dec = X_dec[-MAX_SAMPLES:]\n",
    "    y_seq = y_seq[-MAX_SAMPLES:]\n",
    "\n",
    "# --- Modell ---\n",
    "input_dim = X_enc.shape[2]\n",
    "enc_inputs = Input(shape=(SEQ_LEN, input_dim), name=\"encoder_input\")\n",
    "_, state_h, state_c = LSTM(LSTM_UNITS, return_state=True, name=\"encoder_lstm\")(enc_inputs)\n",
    "dec_inputs = Input(shape=(SEQ_LEN, 1), name=\"decoder_input\")\n",
    "dec_outputs = LSTM(LSTM_UNITS, return_sequences=True, name=\"decoder_lstm\")(dec_inputs, initial_state=[state_h, state_c])\n",
    "outputs = Dense(1, activation=None)(dec_outputs)  # keine Aktivierung f√ºr Regression\n",
    "\n",
    "model = Model([enc_inputs, dec_inputs], outputs)\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss=\"mae\")\n",
    "\n",
    "# --- MLflow ---\n",
    "mlflow.set_tracking_uri(f\"file://{mlflow_tracking_dir}\")\n",
    "mlflow.set_experiment(\"lstm_seq2seq_multivariate\")\n",
    "\n",
    "if mlflow.active_run():\n",
    "    mlflow.end_run()\n",
    "\n",
    "with mlflow.start_run():\n",
    "    mlflow.log_params({\n",
    "        \"model_type\": \"lstm_seq2seq\",\n",
    "        \"seq_len\": SEQ_LEN,\n",
    "        \"lstm_units\": LSTM_UNITS,\n",
    "        \"epochs\": EPOCHS,\n",
    "        \"batch_size\": BATCH_SIZE,\n",
    "        \"input_dim\": input_dim,\n",
    "        \"features\": FEATURES,\n",
    "        \"target\": \"log1p(unit_sales)\"\n",
    "    })\n",
    "\n",
    "    history = model.fit(\n",
    "        [X_enc, X_dec],\n",
    "        y_seq,\n",
    "        epochs=EPOCHS,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        callbacks=[\n",
    "            EarlyStopping(monitor=\"loss\", patience=3, restore_best_weights=True),\n",
    "            ReduceLROnPlateau(monitor=\"loss\", factor=0.5, patience=2)\n",
    "        ],\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    mlflow.log_metric(\"final_loss\", float(history.history[\"loss\"][-1]))\n",
    "\n",
    "    # --- Forecast ---\n",
    "    X_pred_enc = X_enc[-1:]\n",
    "    X_pred_dec = np.zeros((1, SEQ_LEN, 1), dtype=\"float32\")\n",
    "    X_pred_dec[:, 0, 0] = X_pred_enc[:, -1, 0]\n",
    "    pred_scaled = model.predict([X_pred_enc, X_pred_dec])[0]\n",
    "    pred_log1p = target_scaler.inverse_transform(pred_scaled).flatten()\n",
    "    forecast = np.expm1(pred_log1p)  # r√ºcktransformieren\n",
    "\n",
    "    # --- Plot ---\n",
    "    future_dates = pd.date_range(df[\"date\"].max() + pd.Timedelta(days=1), periods=SEQ_LEN)\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.plot(future_dates, forecast, \"r--\", label=\"Forecast\", linewidth=2)\n",
    "    plt.title(\"Forecast (Next 90 Days)\")\n",
    "    plt.xlabel(\"Date\")\n",
    "    plt.ylabel(\"Unit Sales\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plot_path = \"forecast_plot.png\"\n",
    "    plt.savefig(plot_path)\n",
    "    mlflow.log_artifact(plot_path)\n",
    "    plt.close()\n",
    "\n",
    "    # --- Modell & Scaler speichern ---\n",
    "    model_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    scaler_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    model.save(model_path)\n",
    "    joblib.dump(target_scaler, scaler_path)\n",
    "    mlflow.log_artifact(str(model_path))\n",
    "    mlflow.log_artifact(str(scaler_path))\n",
    "\n",
    "    print(\"‚úÖ Training abgeschlossen. Modell gespeichert und Forecast erzeugt.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de8c89bc",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import mlflow\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import importlib\n",
    "import app.config as cfg\n",
    "importlib.reload(cfg)\n",
    "\n",
    "# --- Parameter ---\n",
    "SEQ_LEN = cfg.SEQ_LEN\n",
    "FEATURES = ['unit_sales'] + cfg.FEATURE_COLS\n",
    "MAX_SAMPLES = 1000\n",
    "\n",
    "# --- Pfade ---\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "data_path = os.path.join(project_root, cfg.PREPARED_DATA_FILE)\n",
    "mlflow_tracking_dir_lstm = os.path.join(project_root, \"mlruns/lstm_global\")\n",
    "\n",
    "# --- Daten laden ---\n",
    "df = pd.read_csv(data_path, parse_dates=[\"date\"])\n",
    "df = df[(df[\"unit_sales\"] >= 0) & (df[\"date\"] <= cfg.CUTOFF_DATE)].sort_values(\"date\").reset_index(drop=True)\n",
    "available_features = [f for f in FEATURES if f in df.columns]\n",
    "missing_features = [f for f in FEATURES if f not in df.columns]\n",
    "\n",
    "if missing_features:\n",
    "    print(f\"‚ö†Ô∏è Warnung: Diese Features fehlen in den Daten und werden ignoriert: {missing_features}\")\n",
    "\n",
    "# Skalierung nur auf existierende Spalten anwenden\n",
    "scalers = {f: MinMaxScaler() for f in available_features}\n",
    "scaled_features = np.stack(\n",
    "    [scalers[f].fit_transform(df[[f]]).flatten() for f in available_features],\n",
    "    axis=-1\n",
    ").astype(\"float32\")\n",
    "\n",
    "\n",
    "# --- Sequenzdaten erzeugen ---\n",
    "def make_multivariate_seq2seq(data, n_in, n_out):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - n_in - n_out + 1):\n",
    "        X.append(data[i : i + n_in])\n",
    "        y.append(data[i + n_in : i + n_in + n_out, 0])  # unit_sales als Target\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "X_raw, y_raw = make_multivariate_seq2seq(scaled_features, SEQ_LEN, SEQ_LEN)\n",
    "\n",
    "# --- Daten vorbereiten ---\n",
    "X_enc = X_raw.astype(\"float32\")\n",
    "X_dec = np.zeros((len(X_raw), SEQ_LEN, 1), dtype=\"float32\")\n",
    "X_dec[:, 1:, :] = y_raw[:, :-1].reshape(len(y_raw), SEQ_LEN - 1, 1)\n",
    "y_seq = y_raw.reshape(len(y_raw), SEQ_LEN, 1).astype(\"float32\")\n",
    "\n",
    "# --- Begrenze Datensatzgr√∂√üe ---\n",
    "if len(X_enc) > MAX_SAMPLES:\n",
    "    X_enc = X_enc[-MAX_SAMPLES:]\n",
    "    X_dec = X_dec[-MAX_SAMPLES:]\n",
    "    y_seq = y_seq[-MAX_SAMPLES:]\n",
    "\n",
    "# --- Modell ---\n",
    "input_dim = X_enc.shape[2]\n",
    "enc_inputs = Input(shape=(SEQ_LEN, input_dim), name=\"encoder_input\")\n",
    "_, state_h, state_c = LSTM(cfg.LSTM_PARAMS[\"lstm_units\"], return_state=True, name=\"encoder_lstm\")(enc_inputs)\n",
    "\n",
    "dec_inputs = Input(shape=(SEQ_LEN, 1), name=\"decoder_input\")\n",
    "dec_lstm = LSTM(cfg.LSTM_PARAMS[\"lstm_units\"], return_sequences=True, name=\"decoder_lstm\")(\n",
    "    dec_inputs, initial_state=[state_h, state_c]\n",
    ")\n",
    "outputs = Dense(1, activation=\"relu\")(dec_lstm)\n",
    "\n",
    "model = Model([enc_inputs, dec_inputs], outputs)\n",
    "model.compile(optimizer=Adam(1e-3), loss=\"mae\")\n",
    "\n",
    "# --- MLflow Logging ---\n",
    "mlflow.set_tracking_uri(f\"file://{mlflow_tracking_dir_lstm}\")\n",
    "mlflow.set_experiment(\"global_lstm_seq2seq_forecast\")\n",
    "\n",
    "if mlflow.active_run():\n",
    "    mlflow.end_run()\n",
    "\n",
    "with mlflow.start_run():\n",
    "    mlflow.log_params({\n",
    "        \"model_type\": \"lstm_seq2seq_multivariate\",\n",
    "        \"seq_len\": SEQ_LEN,\n",
    "        \"lstm_units\": cfg.LSTM_PARAMS[\"lstm_units\"],\n",
    "        \"epochs\": cfg.LSTM_PARAMS[\"epochs\"],\n",
    "        \"batch_size\": cfg.LSTM_PARAMS[\"batch_size\"],\n",
    "        \"input_dim\": input_dim,\n",
    "        \"sample_size\": len(X_enc)\n",
    "    })\n",
    "\n",
    "    # Training\n",
    "    history = model.fit(\n",
    "        [X_enc, X_dec], y_seq,\n",
    "        epochs=cfg.LSTM_PARAMS[\"epochs\"],\n",
    "        batch_size=cfg.LSTM_PARAMS[\"batch_size\"],\n",
    "        callbacks=[\n",
    "            EarlyStopping(monitor=\"loss\", patience=cfg.LSTM_PARAMS[\"patience\"], restore_best_weights=True),\n",
    "            ReduceLROnPlateau(monitor=\"loss\", factor=0.5, patience=2)\n",
    "        ],\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    final_loss = history.history[\"loss\"][-1]\n",
    "    mlflow.log_metric(\"final_loss\", final_loss)\n",
    "\n",
    "    # Forecast erzeugen\n",
    "    X_pred_enc = X_enc[-1:].copy()\n",
    "    X_pred_dec = np.zeros((1, SEQ_LEN, 1), dtype=\"float32\")\n",
    "\n",
    "    pred_scaled = model.predict([X_pred_enc, X_pred_dec])[0]\n",
    "    pred_unit_sales = scalers[\"unit_sales\"].inverse_transform(pred_scaled).flatten()\n",
    "\n",
    "    future_dates = pd.date_range(df[\"date\"].max() + pd.Timedelta(days=1), periods=SEQ_LEN)\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.plot(df[\"date\"][-SEQ_LEN:], df[\"unit_sales\"][-SEQ_LEN:], label=\"Last Seen\", linewidth=2)\n",
    "    plt.plot(future_dates, pred_unit_sales, \"r--\", label=\"Forecast\", linewidth=2)\n",
    "    plt.title(\"Multivariate LSTM Seq2Seq Forecast\")\n",
    "    plt.xlabel(\"Date\")\n",
    "    plt.ylabel(\"Unit Sales\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    plot_path = \"lstm_multivariate_forecast_plot.png\"\n",
    "    plt.savefig(plot_path)\n",
    "    mlflow.log_artifact(plot_path)\n",
    "    plt.close()\n",
    "\n",
    "    # Speichern\n",
    "    model_path = project_root / cfg.LSTM_ARCHIVE_DIR / cfg.LSTM_GLOBAL_MODEL\n",
    "    scaler_path = project_root / cfg.SCALER_ARCHIVE_DIR / cfg.LSTM_GLOBAL_SCALER\n",
    "    model_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    scaler_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    model.save(model_path)\n",
    "    joblib.dump(scalers[\"unit_sales\"], scaler_path)\n",
    "\n",
    "    mlflow.log_artifact(str(model_path))\n",
    "    mlflow.log_artifact(str(scaler_path))\n",
    "\n",
    "    print(f\"‚úÖ Multivariate LSTM (SEQ_LEN={SEQ_LEN}) gespeichert & in MLflow geloggt.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc7ba0a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìå Training model for store 24 and item 220435\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-26 12:15:42.628014: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M2\n",
      "2025-05-26 12:15:42.628257: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 8.00 GB\n",
      "2025-05-26 12:15:42.628276: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 2.67 GB\n",
      "2025-05-26 12:15:42.628597: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2025-05-26 12:15:42.629077: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-26 12:15:44.230204: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:117] Plugin optimizer for device_type GPU is enabled.\n",
      "2025-05-26 12:15:44.237729: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:961] PluggableGraphOptimizer failed: INVALID_ARGUMENT: Failed to deserialize the `graph_buf`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m6/6\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 168ms/step - loss: 0.5207 - val_loss: 0.5018 - learning_rate: 0.0010\n",
      "Epoch 2/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - loss: 0.5199 - val_loss: 0.4984 - learning_rate: 0.0010\n",
      "Epoch 3/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - loss: 0.4614 - val_loss: 0.1657 - learning_rate: 0.0010\n",
      "Epoch 4/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - loss: 0.1685 - val_loss: 0.1310 - learning_rate: 0.0010\n",
      "Epoch 5/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - loss: 0.1317 - val_loss: 0.1294 - learning_rate: 0.0010\n",
      "Epoch 6/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - loss: 0.1263 - val_loss: 0.1300 - learning_rate: 0.0010\n",
      "Epoch 7/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - loss: 0.1185 - val_loss: 0.1123 - learning_rate: 0.0010\n",
      "Epoch 8/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 0.1135 - val_loss: 0.1163 - learning_rate: 0.0010\n",
      "Epoch 9/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - loss: 0.1082 - val_loss: 0.1110 - learning_rate: 0.0010\n",
      "Epoch 10/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - loss: 0.1078 - val_loss: 0.1118 - learning_rate: 0.0010\n",
      "Epoch 11/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - loss: 0.1072 - val_loss: 0.1103 - learning_rate: 0.0010\n",
      "Epoch 12/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - loss: 0.1063 - val_loss: 0.1090 - learning_rate: 0.0010\n",
      "Epoch 13/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - loss: 0.1067 - val_loss: 0.1098 - learning_rate: 0.0010\n",
      "Epoch 14/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - loss: 0.1047 - val_loss: 0.1100 - learning_rate: 0.0010\n",
      "Epoch 15/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - loss: 0.1058 - val_loss: 0.1098 - learning_rate: 0.0010\n",
      "Epoch 16/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - loss: 0.1044 - val_loss: 0.1099 - learning_rate: 5.0000e-04\n",
      "Epoch 17/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - loss: 0.1052 - val_loss: 0.1094 - learning_rate: 5.0000e-04\n",
      "‚úÖ Seq2Seq LSTM trained | MAE: 25.84, R¬≤: -2.817\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['../model/scaler/archive/scaler_seq2seq_store24_item220435.pkl']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Parameters\n",
    "n_input  = cfg.SEQ_LEN   # input sequence length\n",
    "n_output = cfg.SEQ_LEN   # forecast horizon\n",
    "\n",
    "# fixed store/item\n",
    "print(f\"üìå Training model for store {store_id} and item {item_id}\")\n",
    "subset = (\n",
    "    df_train\n",
    "    .loc[(df_train.store_nbr==store_id)&(df_train.item_nbr==item_id)]\n",
    "    .sort_values(\"date\")\n",
    "    .set_index(\"date\")[[\"unit_sales\"]]\n",
    "    .astype(float)\n",
    ")\n",
    "scaler = MinMaxScaler()\n",
    "subset[\"scaled\"] = scaler.fit_transform(subset[[\"unit_sales\"]])\n",
    "\n",
    "# Build seq2seq sequences\n",
    "def make_seq2seq_data(series, n_in, n_out):\n",
    "    X, y = [], []\n",
    "    for i in range(len(series) - n_in - n_out):\n",
    "        X.append(series[i : i + n_in])\n",
    "        y.append(series[i + n_in : i + n_in + n_out])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "series = subset[\"scaled\"].values\n",
    "X_all, y_all = make_seq2seq_data(series, n_input, n_output)\n",
    "dates_all = subset.index[n_input + n_output :]\n",
    "\n",
    "# Split train/test\n",
    "cutoff = pd.to_datetime(cfg.CUTOFF_DATE)\n",
    "split_idx = np.sum(dates_all <= cutoff)\n",
    "X_train, X_test = X_all[:split_idx], X_all[split_idx:]\n",
    "y_train, y_test = y_all[:split_idx], y_all[split_idx:]\n",
    "\n",
    "# Reshape and prepare decoder inputs (Teacher Forcing)\n",
    "X_train = X_train[..., np.newaxis]\n",
    "X_test  = X_test[...,  np.newaxis]\n",
    "dec_in_train = np.zeros((len(X_train), n_output, 1))\n",
    "dec_in_train[:,1:,:] = y_train[:,:-1][...,None]\n",
    "dec_in_test  = np.zeros((len(X_test), n_output, 1))\n",
    "\n",
    "# Build a deeper Seq2Seq model\n",
    "def build_seq2seq_model(n_in, n_out):\n",
    "    # Encoder\n",
    "    enc_inputs = Input(shape=(n_in,1), name=\"encoder_input\")\n",
    "    e = LSTM(64, return_sequences=True)(enc_inputs)\n",
    "    _, state_h, state_c = LSTM(64, return_state=True)(e)\n",
    "    # Decoder\n",
    "    dec_inputs = Input(shape=(n_out,1), name=\"decoder_input\")\n",
    "    d = LSTM(64, return_sequences=True)(dec_inputs, initial_state=[state_h, state_c])\n",
    "    d = LSTM(64, return_sequences=True)(d)\n",
    "    outputs = Dense(1, activation=\"relu\")(d)\n",
    "    model = Model([enc_inputs, dec_inputs], outputs)\n",
    "    model.compile(optimizer=Adam(1e-3), loss=\"mae\")\n",
    "    return model\n",
    "\n",
    "model = build_seq2seq_model(n_input, n_output)\n",
    "\n",
    "# Setup callbacks\n",
    "early     = EarlyStopping(monitor=\"val_loss\", patience=5, restore_best_weights=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=3)\n",
    "\n",
    "# Train & log with MLflow\n",
    "mlflow.set_tracking_uri(\"../mlruns/xgb_seq2seq\")\n",
    "\n",
    "if mlflow.active_run(): mlflow.end_run()\n",
    "mlflow.set_experiment(\"lstm_seq2seq_forecast\")\n",
    "with mlflow.start_run():\n",
    "    # Log params\n",
    "    mlflow.log_param(\"store_id\", store_id)\n",
    "    mlflow.log_param(\"item_id\", item_id)\n",
    "    mlflow.log_param(\"n_input\", n_input)\n",
    "    mlflow.log_param(\"n_output\", n_output)\n",
    "    # Train\n",
    "    model.fit(\n",
    "        [X_train, dec_in_train],\n",
    "        y_train[...,None],\n",
    "        validation_split=0.1,\n",
    "        epochs=100,\n",
    "        batch_size=32,\n",
    "        callbacks=[early, reduce_lr],\n",
    "        verbose=1\n",
    "    )\n",
    "    # Predict\n",
    "    y_pred = model.predict([X_test, dec_in_test], verbose=0)\n",
    "    y_pred = np.maximum(y_pred, 0)\n",
    "    # Inverse transform\n",
    "    y_pred_inv = scaler.inverse_transform(y_pred[:,:,0])\n",
    "    y_test_inv = scaler.inverse_transform(y_test)\n",
    "    # Metrics\n",
    "    mae = mean_absolute_error(y_test_inv, y_pred_inv)\n",
    "    r2  = r2_score(y_test_inv.flatten(), y_pred_inv.flatten())\n",
    "    mlflow.log_metric(\"mae\", mae)\n",
    "    mlflow.log_metric(\"r2_score\", r2)\n",
    "    # Signature & Input-Example\n",
    "    n_ex = min(5, len(X_test))\n",
    "    input_example = {\n",
    "        \"encoder_input\": X_test[:n_ex],\n",
    "        \"decoder_input\": dec_in_test[:n_ex]\n",
    "    }\n",
    "    preds_example = y_pred[:n_ex]\n",
    "    signature = infer_signature(input_example, preds_example)\n",
    "    mlflow.keras.log_model(\n",
    "        model,\n",
    "        artifact_path=\"lstm_seq2seq_model\",\n",
    "        signature=signature\n",
    "        # , input_example=input_example\n",
    "    )\n",
    "    print(f\"‚úÖ Seq2Seq LSTM trained | MAE: {mae:.2f}, R¬≤: {r2:.3f}\")\n",
    "\n",
    "# Save artifacts\n",
    "os.makedirs(\"../model/lstm/archive\", exist_ok=True)\n",
    "model.save(f\"../model/lstm/archive/lstm_seq2seq_store{store_id}_item{item_id}.keras\")\n",
    "\n",
    "os.makedirs(\"../model/scaler/archive\", exist_ok=True)\n",
    "joblib.dump(scaler, f\"../model/scaler/archive/scaler_seq2seq_store{store_id}_item{item_id}.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e4cfdaa",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'store_item_pairs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 30\u001b[0m\n\u001b[1;32m     27\u001b[0m mlflow\u001b[38;5;241m.\u001b[39mset_tracking_uri(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfile://\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmlflow_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     28\u001b[0m mlflow\u001b[38;5;241m.\u001b[39mset_experiment(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlstm_seq2seq_forecast_global\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 30\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m store_id, item_id \u001b[38;5;129;01min\u001b[39;00m store_item_pairs:\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124müì¶ Training for Store: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstore_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | Item: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mitem_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;66;03m# --- Daten vorbereiten ---\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'store_item_pairs' is not defined"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import joblib\n",
    "import mlflow\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import app.config as cfg\n",
    "\n",
    "# --- Konfiguration und Pfade ---\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "data_path = os.path.join(project_root, cfg.PREPARED_DATA_FILE)\n",
    "model_dir = Path(project_root, cfg.LSTM_ARCHIVE_DIR)\n",
    "scaler_dir = Path(project_root, cfg.SCALER_ARCHIVE_DIR)\n",
    "mlflow_dir = os.path.join(project_root, cfg.MLFLOW_LSTM_SEQ)\n",
    "\n",
    "\n",
    "\n",
    "# --- Setup f√ºr MLflow ---\n",
    "os.makedirs(mlflow_dir, exist_ok=True)\n",
    "mlflow.set_tracking_uri(f\"file://{mlflow_dir}\")\n",
    "mlflow.set_experiment(\"lstm_seq2seq_forecast_global\")\n",
    "\n",
    "for store_id, item_id in store_item_pairs:\n",
    "    print(f\"üì¶ Training for Store: {store_id} | Item: {item_id}\")\n",
    "    \n",
    "    # --- Daten vorbereiten ---\n",
    "    df = pd.read_csv(data_path, parse_dates=[\"date\"])\n",
    "    df = df[df['unit_sales'] >= 0]\n",
    "    df = df[(df['store_nbr'] == store_id) & (df['item_nbr'] == item_id)].sort_values('date')\n",
    "\n",
    "    if len(df) < 3 * cfg.SEQ_LEN:\n",
    "        print(f\"‚ùå Not enough data for Store {store_id} Item {item_id}\")\n",
    "        continue\n",
    "\n",
    "    cutoff = pd.to_datetime(cfg.CUTOFF_DATE)\n",
    "    train_df = df[df['date'] <= cutoff]\n",
    "    scaler = MinMaxScaler()\n",
    "    df['unit_sales_scaled'] = scaler.fit_transform(df[['unit_sales']])\n",
    "\n",
    "    # --- Sequenz-Erstellung ---\n",
    "    def make_seq2seq_data(series, n_in, n_out):\n",
    "        X, y = [], []\n",
    "        for i in range(len(series) - n_in - n_out):\n",
    "            X.append(series[i:i + n_in])\n",
    "            y.append(series[i + n_in:i + n_in + n_out])\n",
    "        return np.array(X), np.array(y)\n",
    "\n",
    "    series = df['unit_sales_scaled'].values\n",
    "    X_all, y_all = make_seq2seq_data(series, cfg.SEQ_LEN, cfg.SEQ_LEN)\n",
    "    dates_all = df['date'].iloc[cfg.SEQ_LEN * 2:].values\n",
    "\n",
    "    # --- Train/Test Split ---\n",
    "    split_idx = np.sum(pd.to_datetime(dates_all) <= cutoff)\n",
    "    X_train, X_test = X_all[:split_idx], X_all[split_idx:]\n",
    "    y_train, y_test = y_all[:split_idx], y_all[split_idx:]\n",
    "\n",
    "    # --- Decoder Inputs ---\n",
    "    X_train = X_train[..., np.newaxis]\n",
    "    X_test = X_test[..., np.newaxis]\n",
    "    dec_in_train = np.zeros((len(X_train), cfg.SEQ_LEN, 1))\n",
    "    dec_in_train[:, 1:, :] = y_train[:, :-1][..., None]\n",
    "    dec_in_test = np.zeros((len(X_test), cfg.SEQ_LEN, 1))\n",
    "\n",
    "    # --- Modell ---\n",
    "    def build_seq2seq_model(n_in, n_out):\n",
    "        enc_inputs = Input(shape=(n_in, 1), name=\"encoder_input\")\n",
    "        e = LSTM(64, return_sequences=True)(enc_inputs)\n",
    "        _, state_h, state_c = LSTM(64, return_state=True)(e)\n",
    "        dec_inputs = Input(shape=(n_out, 1), name=\"decoder_input\")\n",
    "        d = LSTM(64, return_sequences=True)(dec_inputs, initial_state=[state_h, state_c])\n",
    "        d = LSTM(64, return_sequences=True)(d)\n",
    "        outputs = Dense(1, activation=\"relu\")(d)\n",
    "        model = Model([enc_inputs, dec_inputs], outputs)\n",
    "        model.compile(optimizer=Adam(1e-3), loss=\"mae\")\n",
    "        return model\n",
    "\n",
    "    model = build_seq2seq_model(cfg.SEQ_LEN, cfg.SEQ_LEN)\n",
    "\n",
    "    early = EarlyStopping(monitor=\"val_loss\", patience=5, restore_best_weights=True)\n",
    "    reduce_lr = ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=3)\n",
    "\n",
    "    if mlflow.active_run():\n",
    "        mlflow.end_run()\n",
    "\n",
    "    with mlflow.start_run():\n",
    "        mlflow.log_params({\n",
    "            \"store_id\": store_id,\n",
    "            \"item_id\": item_id,\n",
    "            \"seq_len\": cfg.SEQ_LEN,\n",
    "            \"lstm_units\": 64,\n",
    "            \"batch_size\": 32,\n",
    "            \"epochs\": 100,\n",
    "            \"cutoff_date\": cfg.CUTOFF_DATE.strftime('%Y-%m-%d'),\n",
    "            \"model_type\": \"lstm_seq2seq\"\n",
    "        })\n",
    "\n",
    "        # --- Training ---\n",
    "        model.fit(\n",
    "            [X_train, dec_in_train],\n",
    "            y_train[..., None],\n",
    "            validation_split=0.1,\n",
    "            epochs=100,\n",
    "            batch_size=32,\n",
    "            callbacks=[early, reduce_lr],\n",
    "            verbose=1\n",
    "        )\n",
    "\n",
    "        # --- Evaluation ---\n",
    "        y_pred = model.predict([X_test, dec_in_test], verbose=0)\n",
    "        y_pred = np.maximum(y_pred, 0)\n",
    "        y_pred_inv = scaler.inverse_transform(y_pred[:, :, 0])\n",
    "        y_test_inv = scaler.inverse_transform(y_test)\n",
    "\n",
    "        mae = mean_absolute_error(y_test_inv, y_pred_inv)\n",
    "        r2 = r2_score(y_test_inv.flatten(), y_pred_inv.flatten())\n",
    "\n",
    "        mlflow.log_metrics({\n",
    "            \"mae\": float(mae),\n",
    "            \"r2_score\": float(r2)\n",
    "        })\n",
    "\n",
    "        # --- Artefakte speichern ---\n",
    "        model_name = f\"lstm_seq2seq_store{store_id}_item{item_id}.keras\"\n",
    "        scaler_name = f\"scaler_seq2seq_store{store_id}_item{item_id}.pkl\"\n",
    "        model_path = model_dir / model_name\n",
    "        scaler_path = scaler_dir / scaler_name\n",
    "\n",
    "        model_dir.mkdir(parents=True, exist_ok=True)\n",
    "        scaler_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        model.save(model_path)\n",
    "        joblib.dump(scaler, scaler_path)\n",
    "\n",
    "        mlflow.log_artifact(str(model_path))\n",
    "        mlflow.log_artifact(str(scaler_path))\n",
    "\n",
    "        # --- Plot Forecast vs Actuals ---\n",
    "        plt.figure(figsize=(10, 4))\n",
    "        sample_size = min(300, len(y_test_inv))\n",
    "        plt.plot(y_test_inv[:sample_size], label=\"Actual\", linewidth=2)\n",
    "        plt.plot(y_pred_inv[:sample_size], label=\"Forecast\", linestyle=\"--\")\n",
    "        plt.title(f\"Forecast vs. Actual for Store {store_id} Item {item_id}\")\n",
    "        plt.xlabel(\"Time Step\")\n",
    "        plt.ylabel(\"Unit Sales\")\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.tight_layout()\n",
    "        plot_file = f\"forecast_store{store_id}_item{item_id}.png\"\n",
    "        plt.savefig(plot_file)\n",
    "        mlflow.log_artifact(plot_file)\n",
    "        plt.close()\n",
    "\n",
    "        print(f\"‚úÖ Finished: Store {store_id} | Item {item_id} | MAE: {mae:.2f} | R¬≤: {r2:.3f}\")\n",
    "\n",
    "if mlflow.active_run():\n",
    "    mlflow.end_run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47d77e97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìå Training model for store 24 and item 220435\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-26 13:38:22.254124: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:961] PluggableGraphOptimizer failed: INVALID_ARGUMENT: Failed to deserialize the `graph_buf`.\n",
      "2025-05-26 13:40:22.213693: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:961] PluggableGraphOptimizer failed: INVALID_ARGUMENT: Failed to deserialize the `graph_buf`.\n",
      "2025-05-26 13:50:17.309179: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:961] PluggableGraphOptimizer failed: INVALID_ARGUMENT: Failed to deserialize the `graph_buf`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 664ms/step\n",
      "‚úÖ Model trained | MAE: 8.68, R¬≤: -0.104\n",
      "‚úÖ Saved model to /Users/jennypetschke/VS_Code_Projects/retail_demand_analysis/model/lstm/archive/lstm_default_store24_item220435.keras\n",
      "‚úÖ Saved scaler to /Users/jennypetschke/VS_Code_Projects/retail_demand_analysis/model/scaler/archive/lstm_default_scaler_store24_item220435.pkl\n"
     ]
    }
   ],
   "source": [
    "# --- Imports ---\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "import mlflow\n",
    "from pathlib import Path\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from mlflow.models.signature import infer_signature\n",
    "import app.config as cfg\n",
    "\n",
    "# --- Parameter ---\n",
    "n_steps = cfg.SEQ_LEN\n",
    "EPOCHS = 6\n",
    "BATCH_SIZE = 16\n",
    "DROPOUT = 0.2\n",
    "UNITS = 8\n",
    "LEARNING_RATE = 3e-2\n",
    "\n",
    "# --- Daten laden ---\n",
    "# --- Konfiguration und Pfade ---\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "data_path = os.path.join(project_root, cfg.PREPARED_DATA_FILE)\n",
    "model_dir = Path(project_root, cfg.LSTM_ARCHIVE_DIR)\n",
    "scaler_dir = Path(project_root, cfg.SCALER_ARCHIVE_DIR)\n",
    "mlflow_dir = os.path.join(project_root, cfg.MLFLOW_LSTM_SEQ)\n",
    "df_train = pd.read_csv(data_path, parse_dates=[\"date\"])\n",
    "df_train = df_train[df_train[\"unit_sales\"] >= 0]\n",
    "\n",
    "# --- Cutoff ---\n",
    "cutoff_date = cfg.CUTOFF_DATE\n",
    "\n",
    "# --- Statistische Auswahl der besten Kombination ---\n",
    "df_2013 = df_train[df_train['date'].dt.year == 2013]\n",
    "stats = (\n",
    "    df_2013.groupby(['store_nbr', 'item_nbr'])['unit_sales']\n",
    "    .agg(count_days='count', mean_sales='mean', std_sales='std', positive_days=lambda x: (x>0).sum())\n",
    "    .reset_index()\n",
    ")\n",
    "stats = stats[stats['count_days'] >= 300]\n",
    "stats['score'] = stats['mean_sales'] / (stats['std_sales'] + 1e-5)\n",
    "\n",
    "# Beste Kombination w√§hlen\n",
    "best_combo = stats.sort_values('score', ascending=False).iloc[0]\n",
    "store_id = int(best_combo['store_nbr'])\n",
    "item_id = int(best_combo['item_nbr'])\n",
    "\n",
    "print(f\"üìå Training model for store {store_id} and item {item_id}\")\n",
    "\n",
    "# --- Store-Item-Subset vorbereiten ---\n",
    "df = (\n",
    "    df_train\n",
    "    .loc[(df_train.store_nbr == store_id) & (df_train.item_nbr == item_id)]\n",
    "    .sort_values(\"date\")\n",
    "    .set_index(\"date\")[[\"unit_sales\"]]\n",
    "    .astype(float)\n",
    ")\n",
    "\n",
    "# --- Skalierung ---\n",
    "scaler = MinMaxScaler()\n",
    "df[\"unit_sales_scaled\"] = scaler.fit_transform(df[[\"unit_sales\"]])\n",
    "series = df[\"unit_sales_scaled\"].values\n",
    "\n",
    "# --- Sequenzen ---\n",
    "def create_sequences(data, n_steps):\n",
    "    X, y = [], []\n",
    "    for i in range(n_steps, len(data)):\n",
    "        X.append(data[i-n_steps:i])\n",
    "        y.append(data[i])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "X, y = create_sequences(series, n_steps)\n",
    "dates = df.index[n_steps:]\n",
    "split_point = np.sum(dates <= cutoff_date)\n",
    "\n",
    "X_train, X_test = X[:split_point], X[split_point:]\n",
    "y_train, y_test = y[:split_point], y[split_point:]\n",
    "\n",
    "X_train = X_train.reshape((-1, n_steps, 1))\n",
    "X_test  = X_test.reshape((-1, n_steps, 1))\n",
    "\n",
    "# --- Modell definieren ---\n",
    "model = Sequential([\n",
    "    Input(shape=(n_steps, 1)),\n",
    "    LSTM(UNITS, activation=\"relu\"),\n",
    "    Dropout(DROPOUT),\n",
    "    Dense(1),\n",
    "])\n",
    "model.compile(optimizer=Adam(learning_rate=LEARNING_RATE), loss=\"mae\")\n",
    "\n",
    "# --- MLflow Setup ---\n",
    "mlflow.set_tracking_uri(f\"file://{os.path.join(project_root, cfg.MLFLOW_LSTM_SEQ)}\")\n",
    "mlflow.set_experiment(\"lstm_unit_sales_forecast\")\n",
    "\n",
    "if mlflow.active_run():\n",
    "    mlflow.end_run()\n",
    "\n",
    "with mlflow.start_run():\n",
    "    # Parameter\n",
    "    mlflow.log_params({\n",
    "        \"store_id\": store_id,\n",
    "        \"item_id\": item_id,\n",
    "        \"n_steps\": n_steps,\n",
    "        \"lstm_units\": UNITS,\n",
    "        \"dropout_rate\": DROPOUT,\n",
    "        \"optimizer\": \"adam\",\n",
    "        \"learning_rate\": LEARNING_RATE\n",
    "    })\n",
    "\n",
    "    # --- Training ---\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        epochs=EPOCHS,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        validation_split=0.1,\n",
    "        verbose=0\n",
    "    )\n",
    "\n",
    "    # --- Evaluation ---\n",
    "    y_pred = model.predict(X_test, verbose=0)\n",
    "    y_pred_inv = scaler.inverse_transform(y_pred)\n",
    "    y_test_inv = scaler.inverse_transform(y_test.reshape(-1, 1))\n",
    "\n",
    "    mae = mean_absolute_error(y_test_inv, y_pred_inv)\n",
    "    r2 = r2_score(y_test_inv, y_pred_inv)\n",
    "\n",
    "    mlflow.log_metric(\"mae\", mae)\n",
    "    mlflow.log_metric(\"r2_score\", r2)\n",
    "\n",
    "    # --- Signature ---\n",
    "    X_ex = X_train[:3]\n",
    "    y_ex = model.predict(X_ex)\n",
    "    sig = infer_signature(X_ex, y_ex)\n",
    "\n",
    "    mlflow.keras.log_model(model, artifact_path=\"lstm_model\", signature=sig)\n",
    "\n",
    "    print(f\"‚úÖ Model trained | MAE: {mae:.2f}, R¬≤: {r2:.3f}\")\n",
    "\n",
    "# --- Speichern ---\n",
    "model_dir = Path(project_root) / cfg.LSTM_ARCHIVE_DIR\n",
    "scaler_dir = Path(project_root) / cfg.SCALER_ARCHIVE_DIR\n",
    "model_dir.mkdir(parents=True, exist_ok=True)\n",
    "scaler_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "model_path = model_dir / f\"lstm_default_store{store_id}_item{item_id}.keras\"\n",
    "scaler_path = scaler_dir / f\"lstm_default_scaler_store{store_id}_item{item_id}.pkl\"\n",
    "\n",
    "model.save(model_path)\n",
    "joblib.dump(scaler, scaler_path)\n",
    "\n",
    "print(f\"‚úÖ Saved model to {model_path}\")\n",
    "print(f\"‚úÖ Saved scaler to {scaler_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "154ec884",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìà Forecast-Plot gespeichert unter: /Users/jennypetschke/VS_Code_Projects/retail_demand_analysis/model/lstm/archive/forecast_plot_store24_item220435.png\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --- Plot erstellen ---\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.plot(dates[split_point:], y_test_inv.flatten(), label='Actual', linewidth=2)\n",
    "plt.plot(dates[split_point:], y_pred_inv.flatten(), 'r--', label='Forecast', linewidth=2)\n",
    "plt.title(f\"Forecast vs Actual ‚Äì Store {store_id}, Item {item_id}\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Unit Sales\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Plot speichern\n",
    "plot_path = model_dir / f\"forecast_plot_store{store_id}_item{item_id}.png\"\n",
    "plt.savefig(plot_path)\n",
    "plt.close()\n",
    "\n",
    "# Plot auch in MLflow loggen\n",
    "mlflow.log_artifact(str(plot_path))\n",
    "\n",
    "print(f\"üìà Forecast-Plot gespeichert unter: {plot_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83abc975",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'keras.wrappers.scikit_learn'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Sequential\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LSTM, Dense, Dropout\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mwrappers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mscikit_learn\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m KerasRegressor\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptimizers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Adam\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mapp\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfig\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mcfg\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'keras.wrappers.scikit_learn'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import RandomizedSearchCV, train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Dropout\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from keras.optimizers import Adam\n",
    "import app.config as cfg\n",
    "\n",
    "# --- Parameter ---\n",
    "SEQ_LEN = cfg.SEQ_LEN\n",
    "\n",
    "# --- Beispielhafte Annahme: Du hast bereits X_raw und y_raw vorbereitet ---\n",
    "# X_raw shape: (samples, timesteps=SEQ_LEN, features)\n",
    "# y_raw shape: (samples,) oder (samples, 1)\n",
    "\n",
    "# --- Skalierung ---\n",
    "scaler = MinMaxScaler()\n",
    "X_2D = X_raw.reshape(-1, X_raw.shape[-1])  # f√ºr Skalierung in 2D\n",
    "X_scaled_2D = scaler.fit_transform(X_2D)\n",
    "X_scaled = X_scaled_2D.reshape(X_raw.shape)\n",
    "\n",
    "# --- Split in Train/Test ---\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_scaled, y_raw, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# --- Modellfunktion ---\n",
    "def create_model(units=50, dropout_rate=0.2, learning_rate=0.001):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(units, activation='relu', input_shape=(X_train.shape[1], X_train.shape[2])))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(optimizer=Adam(learning_rate=learning_rate), loss='mse')\n",
    "    return model\n",
    "\n",
    "# --- Wrapper f√ºr Scikit-Learn ---\n",
    "model = KerasRegressor(build_fn=create_model, verbose=0)\n",
    "\n",
    "# --- Hyperparameter-Raum ---\n",
    "param_grid = {\n",
    "    'units': [50, 100, 150],\n",
    "    'dropout_rate': [0.1, 0.2, 0.3],\n",
    "    'batch_size': [16, 32, 64],\n",
    "    'epochs': [50, 100],\n",
    "    'learning_rate': [0.001, 0.01],\n",
    "}\n",
    "\n",
    "# --- Randomisierte Suche ---\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=model,\n",
    "    param_distributions=param_grid,\n",
    "    n_iter=10,\n",
    "    cv=3,\n",
    "    verbose=2,\n",
    "    scoring='neg_mean_squared_error',\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# --- Training ---\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "# --- Beste Parameter ---\n",
    "best_params_lstm = random_search.best_params_\n",
    "print(\"‚úÖ Best Parameters for LSTM:\", best_params_lstm)\n",
    "\n",
    "model = KerasRegressor(\n",
    "    model=create_model,\n",
    "    optimizer=Adam,\n",
    "    verbose=0\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc9e0b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/05/26 22:28:09 INFO mlflow.tracking.fluent: Experiment with name 'LSTM_per_Store' does not exist. Creating a new experiment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Training Store: 24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-26 22:28:24.601866: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M2\n",
      "2025-05-26 22:28:24.603628: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 8.00 GB\n",
      "2025-05-26 22:28:24.604082: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 2.67 GB\n",
      "2025-05-26 22:28:24.605776: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2025-05-26 22:28:24.606790: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n",
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/rnn/rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n",
      "2025-05-26 22:28:40.601625: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:117] Plugin optimizer for device_type GPU is enabled.\n",
      "2025-05-26 22:28:40.609225: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:961] PluggableGraphOptimizer failed: INVALID_ARGUMENT: Failed to deserialize the `graph_buf`.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 71\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m mlflow\u001b[38;5;241m.\u001b[39mstart_run(run_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstore_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstore_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     70\u001b[0m     early \u001b[38;5;241m=\u001b[39m EarlyStopping(monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m\"\u001b[39m, patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, restore_best_weights\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 71\u001b[0m     history \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mfit(X_train, y_train, batch_size\u001b[38;5;241m=\u001b[39mBATCH_SIZE, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, callbacks\u001b[38;5;241m=\u001b[39m[early], verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     73\u001b[0m     y_pred \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(X_test)\u001b[38;5;241m.\u001b[39mflatten()\n\u001b[1;32m     74\u001b[0m     mse \u001b[38;5;241m=\u001b[39m mean_squared_error(y_test, y_pred)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py:371\u001b[0m, in \u001b[0;36mTensorFlowTrainer.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[1;32m    369\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, iterator \u001b[38;5;129;01min\u001b[39;00m epoch_iterator:\n\u001b[1;32m    370\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m--> 371\u001b[0m     logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_function(iterator)\n\u001b[1;32m    372\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_end(step, logs)\n\u001b[1;32m    373\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop_training:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py:219\u001b[0m, in \u001b[0;36mTensorFlowTrainer._make_function.<locals>.function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfunction\u001b[39m(iterator):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[1;32m    217\u001b[0m         iterator, (tf\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mIterator, tf\u001b[38;5;241m.\u001b[39mdistribute\u001b[38;5;241m.\u001b[39mDistributedIterator)\n\u001b[1;32m    218\u001b[0m     ):\n\u001b[0;32m--> 219\u001b[0m         opt_outputs \u001b[38;5;241m=\u001b[39m multi_step_on_iterator(iterator)\n\u001b[1;32m    220\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m opt_outputs\u001b[38;5;241m.\u001b[39mhas_value():\n\u001b[1;32m    221\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:833\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    830\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    832\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 833\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[1;32m    835\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    836\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:878\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    875\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    876\u001b[0m \u001b[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[1;32m    877\u001b[0m \u001b[38;5;66;03m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[0;32m--> 878\u001b[0m results \u001b[38;5;241m=\u001b[39m tracing_compilation\u001b[38;5;241m.\u001b[39mcall_function(\n\u001b[1;32m    879\u001b[0m     args, kwds, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_config\n\u001b[1;32m    880\u001b[0m )\n\u001b[1;32m    881\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_created_variables:\n\u001b[1;32m    882\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating variables on a non-first call to a function\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    883\u001b[0m                    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m decorated with tf.function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    138\u001b[0m flat_inputs \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(bound_args)\n\u001b[0;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m function\u001b[38;5;241m.\u001b[39m_call_flat(  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m    140\u001b[0m     flat_inputs, captured_inputs\u001b[38;5;241m=\u001b[39mfunction\u001b[38;5;241m.\u001b[39mcaptured_inputs\n\u001b[1;32m    141\u001b[0m )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py:1322\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1318\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1319\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1320\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1321\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1322\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inference_function\u001b[38;5;241m.\u001b[39mcall_preflattened(args)\n\u001b[1;32m   1323\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1324\u001b[0m     args,\n\u001b[1;32m   1325\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1326\u001b[0m     executing_eagerly)\n\u001b[1;32m   1327\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:216\u001b[0m, in \u001b[0;36mAtomicFunction.call_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    215\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 216\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcall_flat(\u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m    217\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:251\u001b[0m, in \u001b[0;36mAtomicFunction.call_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[1;32m    250\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[0;32m--> 251\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mcall_function(\n\u001b[1;32m    252\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname,\n\u001b[1;32m    253\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[1;32m    254\u001b[0m         \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mflat_outputs),\n\u001b[1;32m    255\u001b[0m     )\n\u001b[1;32m    256\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    257\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[1;32m    258\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    259\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[1;32m    260\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[1;32m    261\u001b[0m     )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/tensorflow/python/eager/context.py:1500\u001b[0m, in \u001b[0;36mContext.call_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1498\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[1;32m   1499\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1500\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute(\n\u001b[1;32m   1501\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1502\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[1;32m   1503\u001b[0m       inputs\u001b[38;5;241m=\u001b[39mtensor_inputs,\n\u001b[1;32m   1504\u001b[0m       attrs\u001b[38;5;241m=\u001b[39mattrs,\n\u001b[1;32m   1505\u001b[0m       ctx\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1506\u001b[0m   )\n\u001b[1;32m   1507\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1508\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m   1509\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1510\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1514\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[1;32m   1515\u001b[0m   )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[1;32m     54\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import mlflow\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Dropout\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping\n",
    "import app.config as cfg\n",
    "\n",
    "# --- Setup ---\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "data_path = os.path.join(project_root, cfg.PREPARED_DATA_FILE)\n",
    "model_dir = Path(project_root, cfg.LSTM_ARCHIVE_DIR)\n",
    "scaler_dir = Path(project_root, cfg.SCALER_ARCHIVE_DIR)\n",
    "mlflow_dir = os.path.join(project_root, cfg.MLFLOW_LSTM_SEQ)\n",
    "\n",
    "df = pd.read_csv(data_path, parse_dates=[\"date\"])\n",
    "df = df[df[\"unit_sales\"] >= 0]\n",
    "\n",
    "# --- MLflow Setup ---\n",
    "mlflow.set_tracking_uri(f\"file://{mlflow_dir}\")\n",
    "mlflow.set_experiment(\"LSTM_per_Store\")\n",
    "\n",
    "stores = df[\"store_nbr\"].unique()\n",
    "SEQ_LEN = cfg.SEQ_LEN\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "for store_id in stores:\n",
    "    print(f\"üîÑ Training Store: {store_id}\")\n",
    "    store_df = df[df[\"store_nbr\"] == store_id].sort_values(\"date\").copy()\n",
    "\n",
    "    if len(store_df) < SEQ_LEN * 2:\n",
    "        print(f\"‚ö†Ô∏è  Store {store_id} √ºbersprungen (zu wenig Daten)\")\n",
    "        continue\n",
    "\n",
    "    X_raw = store_df[cfg.FEATURE_COLS].values.astype(np.float32)\n",
    "    y_raw = store_df[cfg.TARGET].values.astype(np.float32)\n",
    "\n",
    "    # Skaliere nur die Features\n",
    "    scaler = MinMaxScaler()\n",
    "    X_scaled = scaler.fit_transform(X_raw)\n",
    "\n",
    "    # Sequenzen\n",
    "    X_seq, y_seq = [], []\n",
    "    for i in range(len(X_scaled) - SEQ_LEN):\n",
    "        X_seq.append(X_scaled[i:i + SEQ_LEN])\n",
    "        y_seq.append(y_raw[i + SEQ_LEN])\n",
    "    X_seq = np.array(X_seq)\n",
    "    y_seq = np.array(y_seq)\n",
    "\n",
    "    # Split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_seq, y_seq, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Modell\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(32, activation='relu', input_shape=(SEQ_LEN, X_seq.shape[2])))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(optimizer=Adam(learning_rate=0.001), loss='mse')\n",
    "\n",
    "    # Training\n",
    "    with mlflow.start_run(run_name=f\"store_{store_id}\"):\n",
    "        early = EarlyStopping(monitor=\"loss\", patience=3, restore_best_weights=True)\n",
    "        history = model.fit(X_train, y_train, batch_size=BATCH_SIZE, epochs=10, callbacks=[early], verbose=0)\n",
    "\n",
    "        y_pred = model.predict(X_test).flatten()\n",
    "        mse = mean_squared_error(y_test, y_pred)\n",
    "        mae = mean_absolute_error(y_test, y_pred)\n",
    "        r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "        # Log\n",
    "        mlflow.log_params({\n",
    "            \"store_nbr\": store_id,\n",
    "            \"units\": 32,\n",
    "            \"dropout\": 0.2,\n",
    "            \"seq_len\": SEQ_LEN,\n",
    "            \"batch_size\": BATCH_SIZE\n",
    "        })\n",
    "        mlflow.log_metrics({\"mse\": mse, \"mae\": mae, \"r2\": r2})\n",
    "\n",
    "        # Artefakte\n",
    "        plot_file = f\"forecast_store{store_id}.png\"\n",
    "        plt.figure(figsize=(10, 4))\n",
    "        plt.plot(y_test[:100], label=\"Actual\")\n",
    "        plt.plot(y_pred[:100], label=\"Forecast\", linestyle=\"--\")\n",
    "        plt.title(f\"Store {store_id}: Forecast vs Actual\")\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(plot_file)\n",
    "        mlflow.log_artifact(plot_file)\n",
    "        plt.close()\n",
    "\n",
    "        # Speicherorte\n",
    "        model_path = model_dir / f\"lstm_optimized_store{store_id}.keras\"\n",
    "        scaler_path = scaler_dir / f\"scaler_optimized_store{store_id}.pkl\"\n",
    "        model.save(model_path)\n",
    "        joblib.dump(scaler, scaler_path)\n",
    "        mlflow.log_artifact(str(model_path))\n",
    "        mlflow.log_artifact(str(scaler_path))\n",
    "\n",
    "        print(f\"‚úÖ Store {store_id} fertig | MSE: {mse:.4f} | R¬≤: {r2:.3f}\")\n",
    "\n",
    "if mlflow.active_run():\n",
    "    mlflow.end_run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc43019e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/rnn/rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n",
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n",
      "2025-05-26 21:16:26.782420: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:961] PluggableGraphOptimizer failed: INVALID_ARGUMENT: Failed to deserialize the `graph_buf`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m  414/12497\u001b[0m \u001b[37m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[1m13:43:05\u001b[0m 4s/step - loss: 3495715072.0000"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 99\u001b[0m\n\u001b[1;32m     96\u001b[0m     mlflow\u001b[38;5;241m.\u001b[39mend_run()\n\u001b[1;32m     98\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m mlflow\u001b[38;5;241m.\u001b[39mstart_run():\n\u001b[0;32m---> 99\u001b[0m     history \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mfit(train_gen, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, callbacks\u001b[38;5;241m=\u001b[39mcallbacks, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    101\u001b[0m     \u001b[38;5;66;03m# Evaluate\u001b[39;00m\n\u001b[1;32m    102\u001b[0m     y_preds \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(test_gen)\u001b[38;5;241m.\u001b[39mflatten()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py:371\u001b[0m, in \u001b[0;36mTensorFlowTrainer.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[1;32m    369\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, iterator \u001b[38;5;129;01min\u001b[39;00m epoch_iterator:\n\u001b[1;32m    370\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m--> 371\u001b[0m     logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_function(iterator)\n\u001b[1;32m    372\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_end(step, logs)\n\u001b[1;32m    373\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop_training:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py:219\u001b[0m, in \u001b[0;36mTensorFlowTrainer._make_function.<locals>.function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfunction\u001b[39m(iterator):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[1;32m    217\u001b[0m         iterator, (tf\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mIterator, tf\u001b[38;5;241m.\u001b[39mdistribute\u001b[38;5;241m.\u001b[39mDistributedIterator)\n\u001b[1;32m    218\u001b[0m     ):\n\u001b[0;32m--> 219\u001b[0m         opt_outputs \u001b[38;5;241m=\u001b[39m multi_step_on_iterator(iterator)\n\u001b[1;32m    220\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m opt_outputs\u001b[38;5;241m.\u001b[39mhas_value():\n\u001b[1;32m    221\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:833\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    830\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    832\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 833\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[1;32m    835\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    836\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:878\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    875\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    876\u001b[0m \u001b[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[1;32m    877\u001b[0m \u001b[38;5;66;03m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[0;32m--> 878\u001b[0m results \u001b[38;5;241m=\u001b[39m tracing_compilation\u001b[38;5;241m.\u001b[39mcall_function(\n\u001b[1;32m    879\u001b[0m     args, kwds, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_config\n\u001b[1;32m    880\u001b[0m )\n\u001b[1;32m    881\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_created_variables:\n\u001b[1;32m    882\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating variables on a non-first call to a function\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    883\u001b[0m                    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m decorated with tf.function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    138\u001b[0m flat_inputs \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(bound_args)\n\u001b[0;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m function\u001b[38;5;241m.\u001b[39m_call_flat(  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m    140\u001b[0m     flat_inputs, captured_inputs\u001b[38;5;241m=\u001b[39mfunction\u001b[38;5;241m.\u001b[39mcaptured_inputs\n\u001b[1;32m    141\u001b[0m )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py:1322\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1318\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1319\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1320\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1321\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1322\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inference_function\u001b[38;5;241m.\u001b[39mcall_preflattened(args)\n\u001b[1;32m   1323\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1324\u001b[0m     args,\n\u001b[1;32m   1325\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1326\u001b[0m     executing_eagerly)\n\u001b[1;32m   1327\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:216\u001b[0m, in \u001b[0;36mAtomicFunction.call_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    215\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 216\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcall_flat(\u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m    217\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:251\u001b[0m, in \u001b[0;36mAtomicFunction.call_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[1;32m    250\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[0;32m--> 251\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mcall_function(\n\u001b[1;32m    252\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname,\n\u001b[1;32m    253\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[1;32m    254\u001b[0m         \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mflat_outputs),\n\u001b[1;32m    255\u001b[0m     )\n\u001b[1;32m    256\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    257\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[1;32m    258\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    259\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[1;32m    260\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[1;32m    261\u001b[0m     )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/tensorflow/python/eager/context.py:1500\u001b[0m, in \u001b[0;36mContext.call_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1498\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[1;32m   1499\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1500\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute(\n\u001b[1;32m   1501\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1502\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[1;32m   1503\u001b[0m       inputs\u001b[38;5;241m=\u001b[39mtensor_inputs,\n\u001b[1;32m   1504\u001b[0m       attrs\u001b[38;5;241m=\u001b[39mattrs,\n\u001b[1;32m   1505\u001b[0m       ctx\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1506\u001b[0m   )\n\u001b[1;32m   1507\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1508\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m   1509\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1510\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1514\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[1;32m   1515\u001b[0m   )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[1;32m     54\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "''''import os\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import mlflow\n",
    "import mlflow.keras\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Dropout\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.utils import Sequence\n",
    "import app.config as cfg\n",
    "\n",
    "# --- Setup ---\n",
    "start_time = time.time()\n",
    "\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "data_path = os.path.join(project_root, cfg.PREPARED_DATA_FILE)\n",
    "model_dir = Path(project_root, cfg.LSTM_ARCHIVE_DIR)\n",
    "scaler_dir = Path(project_root, cfg.SCALER_ARCHIVE_DIR)\n",
    "mlflow_dir = os.path.join(project_root, cfg.MLFLOW_LSTM_SEQ)\n",
    "\n",
    "# --- Daten laden ---\n",
    "df = pd.read_csv(data_path, parse_dates=[\"date\"])\n",
    "df = df[df[\"unit_sales\"] >= 0].copy()\n",
    "df = df.tail(500_000)  # Speicheroptimierung f√ºr gro√üe Daten\n",
    "\n",
    "X_all = df[cfg.FEATURE_COLS].values\n",
    "y_all = df[cfg.TARGET].values\n",
    "\n",
    "# --- Skalieren ---\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X_all)\n",
    "\n",
    "# --- Parameter ---\n",
    "SEQ_LEN = cfg.SEQ_LEN\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# --- Generator Klasse ---\n",
    "class TimeseriesGenerator(Sequence):\n",
    "    def __init__(self, X, y, seq_len, batch_size):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.seq_len = seq_len\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return (len(self.X) - self.seq_len - 1) // self.batch_size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        X_batch = []\n",
    "        y_batch = []\n",
    "        start = idx * self.batch_size\n",
    "        if start + self.batch_size + self.seq_len >= len(self.X):\n",
    "            start = len(self.X) - self.batch_size - self.seq_len - 1\n",
    "        for i in range(start, start + self.batch_size):\n",
    "            X_batch.append(self.X[i:i + self.seq_len])\n",
    "            y_batch.append(self.y[i + self.seq_len])\n",
    "        return np.array(X_batch), np.array(y_batch)\n",
    "\n",
    "# --- Generator Instanzen ---\n",
    "split_idx = int(len(X_scaled) * 0.8)\n",
    "train_gen = TimeseriesGenerator(X_scaled[:split_idx], y_all[:split_idx], SEQ_LEN, BATCH_SIZE)\n",
    "test_gen = TimeseriesGenerator(X_scaled[split_idx:], y_all[split_idx:], SEQ_LEN, BATCH_SIZE)\n",
    "\n",
    "# --- Modell ---\n",
    "def create_model(units=32, dropout_rate=0.2):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(units, activation='relu', input_shape=(SEQ_LEN, X_scaled.shape[1])))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(optimizer=Adam(learning_rate=0.001), loss='mse')\n",
    "    return model\n",
    "\n",
    "model = create_model()\n",
    "\n",
    "# --- Callbacks ---\n",
    "model_ckpt = 'temp_lstm_model.keras'\n",
    "callbacks = [\n",
    "    EarlyStopping(monitor='loss', patience=3, restore_best_weights=True),\n",
    "    ModelCheckpoint(model_ckpt, monitor='loss', save_best_only=True)\n",
    "]\n",
    "\n",
    "# --- MLflow Setup ---\n",
    "os.makedirs(mlflow_dir, exist_ok=True)\n",
    "mlflow.set_tracking_uri(f\"file://{mlflow_dir}\")\n",
    "mlflow.set_experiment(\"LSTM_Global_Optimized\")\n",
    "\n",
    "if mlflow.active_run():\n",
    "    mlflow.end_run()\n",
    "\n",
    "with mlflow.start_run():\n",
    "    history = model.fit(train_gen, epochs=5, callbacks=callbacks, verbose=1, shuffle=False)\n",
    "\n",
    "    # Evaluate\n",
    "    y_preds = model.predict(test_gen).flatten()\n",
    "    y_true = np.array([y_all[split_idx + SEQ_LEN + i] for i in range(len(y_preds))])\n",
    "\n",
    "    mse = mean_squared_error(y_true, y_preds)\n",
    "    mae = mean_absolute_error(y_true, y_preds)\n",
    "    r2 = r2_score(y_true, y_preds)\n",
    "    duration = time.time() - start_time\n",
    "\n",
    "    mlflow.log_metrics({\"mse\": mse, \"mae\": mae, \"r2_score\": r2, \"training_time_sec\": duration})\n",
    "    mlflow.log_params({\"units\": 32, \"dropout_rate\": 0.2, \"seq_len\": SEQ_LEN, \"batch_size\": BATCH_SIZE})\n",
    "\n",
    "    # Plot\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.plot(y_true[:100], label=\"Actual\")\n",
    "    plt.plot(y_preds[:100], label=\"Forecast\", linestyle=\"--\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plot_path = \"lstm_forecast_plot.png\"\n",
    "    plt.savefig(plot_path)\n",
    "    mlflow.log_artifact(plot_path)\n",
    "\n",
    "    # Speichern\n",
    "    final_model_path = model_dir / cfg.LSTM_OPTIMAL_MODEL\n",
    "    final_scaler_path = scaler_dir / cfg.LSTM_OPTIMAL_SCALER\n",
    "\n",
    "    model_dir.mkdir(parents=True, exist_ok=True)\n",
    "    scaler_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    model.save(final_model_path)\n",
    "    joblib.dump(scaler, final_scaler_path)\n",
    "\n",
    "    mlflow.log_artifact(str(final_model_path))\n",
    "    mlflow.log_artifact(str(final_scaler_path))\n",
    "\n",
    "    print(f\"\\u2705 Saved model to {final_model_path}\")\n",
    "    print(f\"\\u2705 Saved scaler to {final_scaler_path}\")\n",
    "    print(f\"\\ud83d\\udcc9 MSE: {mse:.4f} | MAE: {mae:.2f} | R¬≤: {r2:.4f}\")\n",
    "\n",
    "if mlflow.active_run():\n",
    "    mlflow.end_run()''''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecb76625",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-26 22:09:06.319567: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M2\n",
      "2025-05-26 22:09:06.320075: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 8.00 GB\n",
      "2025-05-26 22:09:06.320471: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 2.67 GB\n",
      "2025-05-26 22:09:06.320546: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2025-05-26 22:09:06.320639: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n",
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/rnn/rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n",
      "2025-05-26 22:09:07.384323: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:117] Plugin optimizer for device_type GPU is enabled.\n",
      "2025-05-26 22:09:07.389995: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:961] PluggableGraphOptimizer failed: INVALID_ARGUMENT: Failed to deserialize the `graph_buf`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m   13/29493\u001b[0m \u001b[37m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[1m64:21:54\u001b[0m 8s/step - loss: 42.0595"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 75\u001b[0m\n\u001b[1;32m     72\u001b[0m     mlflow\u001b[38;5;241m.\u001b[39mend_run()\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m mlflow\u001b[38;5;241m.\u001b[39mstart_run():\n\u001b[0;32m---> 75\u001b[0m     history \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mfit(train_gen, epochs\u001b[38;5;241m=\u001b[39mEPOCHS, callbacks\u001b[38;5;241m=\u001b[39mcallbacks, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     77\u001b[0m     y_preds \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(test_gen)\u001b[38;5;241m.\u001b[39mflatten()\n\u001b[1;32m     78\u001b[0m     y_true \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([y_all[split_idx \u001b[38;5;241m+\u001b[39m SEQ_LEN \u001b[38;5;241m+\u001b[39m i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(y_preds))])\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py:371\u001b[0m, in \u001b[0;36mTensorFlowTrainer.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[1;32m    369\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, iterator \u001b[38;5;129;01min\u001b[39;00m epoch_iterator:\n\u001b[1;32m    370\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m--> 371\u001b[0m     logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_function(iterator)\n\u001b[1;32m    372\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_end(step, logs)\n\u001b[1;32m    373\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop_training:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py:219\u001b[0m, in \u001b[0;36mTensorFlowTrainer._make_function.<locals>.function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfunction\u001b[39m(iterator):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[1;32m    217\u001b[0m         iterator, (tf\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mIterator, tf\u001b[38;5;241m.\u001b[39mdistribute\u001b[38;5;241m.\u001b[39mDistributedIterator)\n\u001b[1;32m    218\u001b[0m     ):\n\u001b[0;32m--> 219\u001b[0m         opt_outputs \u001b[38;5;241m=\u001b[39m multi_step_on_iterator(iterator)\n\u001b[1;32m    220\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m opt_outputs\u001b[38;5;241m.\u001b[39mhas_value():\n\u001b[1;32m    221\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:833\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    830\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    832\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 833\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[1;32m    835\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    836\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:878\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    875\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    876\u001b[0m \u001b[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[1;32m    877\u001b[0m \u001b[38;5;66;03m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[0;32m--> 878\u001b[0m results \u001b[38;5;241m=\u001b[39m tracing_compilation\u001b[38;5;241m.\u001b[39mcall_function(\n\u001b[1;32m    879\u001b[0m     args, kwds, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_config\n\u001b[1;32m    880\u001b[0m )\n\u001b[1;32m    881\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_created_variables:\n\u001b[1;32m    882\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating variables on a non-first call to a function\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    883\u001b[0m                    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m decorated with tf.function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    138\u001b[0m flat_inputs \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(bound_args)\n\u001b[0;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m function\u001b[38;5;241m.\u001b[39m_call_flat(  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m    140\u001b[0m     flat_inputs, captured_inputs\u001b[38;5;241m=\u001b[39mfunction\u001b[38;5;241m.\u001b[39mcaptured_inputs\n\u001b[1;32m    141\u001b[0m )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py:1322\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1318\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1319\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1320\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1321\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1322\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inference_function\u001b[38;5;241m.\u001b[39mcall_preflattened(args)\n\u001b[1;32m   1323\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1324\u001b[0m     args,\n\u001b[1;32m   1325\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1326\u001b[0m     executing_eagerly)\n\u001b[1;32m   1327\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:216\u001b[0m, in \u001b[0;36mAtomicFunction.call_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    215\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 216\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcall_flat(\u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m    217\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:251\u001b[0m, in \u001b[0;36mAtomicFunction.call_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[1;32m    250\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[0;32m--> 251\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mcall_function(\n\u001b[1;32m    252\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname,\n\u001b[1;32m    253\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[1;32m    254\u001b[0m         \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mflat_outputs),\n\u001b[1;32m    255\u001b[0m     )\n\u001b[1;32m    256\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    257\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[1;32m    258\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    259\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[1;32m    260\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[1;32m    261\u001b[0m     )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/tensorflow/python/eager/context.py:1500\u001b[0m, in \u001b[0;36mContext.call_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1498\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[1;32m   1499\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1500\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute(\n\u001b[1;32m   1501\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1502\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[1;32m   1503\u001b[0m       inputs\u001b[38;5;241m=\u001b[39mtensor_inputs,\n\u001b[1;32m   1504\u001b[0m       attrs\u001b[38;5;241m=\u001b[39mattrs,\n\u001b[1;32m   1505\u001b[0m       ctx\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1506\u001b[0m   )\n\u001b[1;32m   1507\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1508\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m   1509\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1510\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1514\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[1;32m   1515\u001b[0m   )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[1;32m     54\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "''''import os\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import mlflow\n",
    "import mlflow.keras\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Dropout\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.preprocessing.sequence import TimeseriesGenerator\n",
    "import app.config as cfg\n",
    "\n",
    "del X_scaled, X_all, y_all\n",
    "import gc\n",
    "gc.collect()\n",
    "\n",
    "# --- Setup ---\n",
    "start_time = time.time()\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "data_path = os.path.join(project_root, cfg.PREPARED_DATA_FILE)\n",
    "model_dir = Path(project_root, cfg.LSTM_ARCHIVE_DIR)\n",
    "scaler_dir = Path(project_root, cfg.SCALER_ARCHIVE_DIR)\n",
    "mlflow_dir = os.path.join(project_root, cfg.MLFLOW_LSTM_SEQ)\n",
    "\n",
    "# --- Load Data ---\n",
    "df = pd.read_csv(data_path, parse_dates=[\"date\"])\n",
    "df = df[df[\"unit_sales\"] >= 0].copy()\n",
    "\n",
    "X_all = df[cfg.FEATURE_COLS].values.astype(np.float32)\n",
    "y_all = df[cfg.TARGET].values.astype(np.float32)\n",
    "\n",
    "# --- Scaling ---\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X_all)\n",
    "\n",
    "# --- Parameters ---\n",
    "SEQ_LEN = cfg.SEQ_LEN\n",
    "BATCH_SIZE = 256\n",
    "UNITS = 32\n",
    "DROPOUT = 0.2\n",
    "EPOCHS = 5\n",
    "\n",
    "# --- Train/Test Split ---\n",
    "split_idx = int(len(X_scaled) * 0.8)\n",
    "train_gen = TimeseriesGenerator(X_scaled[:split_idx], y_all[:split_idx], length=SEQ_LEN, batch_size=BATCH_SIZE)\n",
    "test_gen = TimeseriesGenerator(X_scaled[split_idx:], y_all[split_idx:], length=SEQ_LEN, batch_size=BATCH_SIZE)\n",
    "\n",
    "# --- Model Definition ---\n",
    "def create_model():\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(UNITS, activation='relu', input_shape=(SEQ_LEN, X_scaled.shape[1])))\n",
    "    model.add(Dropout(DROPOUT))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(optimizer=Adam(learning_rate=0.001), loss='mse')\n",
    "    return model\n",
    "\n",
    "model = create_model()\n",
    "\n",
    "# --- Callbacks ---\n",
    "callbacks = [\n",
    "    EarlyStopping(monitor='loss', patience=2, restore_best_weights=True)\n",
    "]\n",
    "\n",
    "# --- MLflow Setup ---\n",
    "os.makedirs(mlflow_dir, exist_ok=True)\n",
    "mlflow.set_tracking_uri(f\"file://{mlflow_dir}\")\n",
    "mlflow.set_experiment(\"LSTM_Global_Optimized\")\n",
    "\n",
    "if mlflow.active_run():\n",
    "    mlflow.end_run()\n",
    "\n",
    "with mlflow.start_run():\n",
    "    history = model.fit(train_gen, epochs=EPOCHS, callbacks=callbacks, verbose=1)\n",
    "\n",
    "    y_preds = model.predict(test_gen).flatten()\n",
    "    y_true = np.array([y_all[split_idx + SEQ_LEN + i] for i in range(len(y_preds))])\n",
    "\n",
    "    mse = mean_squared_error(y_true, y_preds)\n",
    "    mae = mean_absolute_error(y_true, y_preds)\n",
    "    r2 = r2_score(y_true, y_preds)\n",
    "    duration = time.time() - start_time\n",
    "\n",
    "    mlflow.log_metrics({\"mse\": mse, \"mae\": mae, \"r2_score\": r2, \"training_time_sec\": duration})\n",
    "    mlflow.log_params({\"units\": UNITS, \"dropout_rate\": DROPOUT, \"seq_len\": SEQ_LEN, \"batch_size\": BATCH_SIZE})\n",
    "\n",
    "    # --- Plot ---\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.plot(y_true[:100], label=\"Actual\")\n",
    "    plt.plot(y_preds[:100], label=\"Forecast\", linestyle=\"--\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plot_path = \"lstm_forecast_plot.png\"\n",
    "    plt.savefig(plot_path)\n",
    "    mlflow.log_artifact(plot_path)\n",
    "    plt.close()\n",
    "\n",
    "    # --- Save model & scaler ---\n",
    "    model_path = model_dir / cfg.LSTM_OPTIMAL_MODEL\n",
    "    scaler_path = scaler_dir / cfg.LSTM_OPTIMAL_SCALER\n",
    "    model_dir.mkdir(parents=True, exist_ok=True)\n",
    "    scaler_dir.mkdir(parents=True, exist_ok=True)\n",
    "    model.save(model_path)\n",
    "    joblib.dump(scaler, scaler_path)\n",
    "    mlflow.log_artifact(str(model_path))\n",
    "    mlflow.log_artifact(str(scaler_path))\n",
    "\n",
    "    print(f\"\\n‚úÖ Model saved: {model_path}\")\n",
    "    print(f\"‚úÖ Scaler saved: {scaler_path}\")\n",
    "    print(f\"üìâ MSE: {mse:.4f} | MAE: {mae:.2f} | R¬≤: {r2:.4f}\")\n",
    "\n",
    "    del X_scaled, X_all, y_all\n",
    "    import gc\n",
    "    gc.collect()\n",
    "\n",
    "if mlflow.active_run():\n",
    "    mlflow.end_run()\n",
    "''''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f4d961",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import mlflow\n",
    "import mlflow.keras\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Dropout\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "import app.config as cfg\n",
    "\n",
    "# --- Setup ---\n",
    "start_time = time.time()\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "data_path = os.path.join(project_root, cfg.PREPARED_DATA_FILE)\n",
    "model_dir = Path(project_root, cfg.LSTM_ARCHIVE_DIR)\n",
    "scaler_dir = Path(project_root, cfg.SCALER_ARCHIVE_DIR)\n",
    "mlflow_dir = os.path.join(project_root, cfg.MLFLOW_LSTM_SEQ)\n",
    "\n",
    "os.makedirs(mlflow_dir, exist_ok=True)\n",
    "mlflow.set_tracking_uri(f\"file://{mlflow_dir}\")\n",
    "mlflow.set_experiment(\"LSTM_Global_Reduzierte_Features\")\n",
    "\n",
    "# --- Daten laden ---\n",
    "df = pd.read_csv(data_path, parse_dates=[\"date\"])\n",
    "df = df[df[\"unit_sales\"] >= 0]\n",
    "\n",
    "# --- Feature-Auswahl ---\n",
    "FEATURES = [\n",
    "    'store_nbr', 'item_nbr', 'onpromotion', 'month',\n",
    "    'unit_sales_7d_avg', 'family_code',\n",
    "    'lag_1', 'lag_7', 'rolling_mean_7'\n",
    "]\n",
    "TARGET = cfg.TARGET\n",
    "SEQ_LEN = cfg.SEQ_LEN\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 5\n",
    "\n",
    "# --- Features & Zielwerte ---\n",
    "X_all = df[FEATURES].astype(np.float32).values\n",
    "y_all = df[TARGET].astype(np.float32).values\n",
    "\n",
    "# --- Skalierung ---\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X_all)\n",
    "\n",
    "# --- Sequenzbildung ---\n",
    "X_seq, y_seq = [], []\n",
    "for i in range(len(X_scaled) - SEQ_LEN):\n",
    "    X_seq.append(X_scaled[i:i + SEQ_LEN])\n",
    "    y_seq.append(y_all[i + SEQ_LEN])\n",
    "X_seq = np.array(X_seq)\n",
    "y_seq = np.array(y_seq)\n",
    "\n",
    "# --- Train/Test Split ---\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_seq, y_seq, test_size=0.2, random_state=42)\n",
    "\n",
    "# --- Modell bauen ---\n",
    "def build_model(input_shape, units=64, dropout=0.2):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(units, activation='relu', input_shape=input_shape))\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(optimizer=Adam(learning_rate=0.001), loss='mse')\n",
    "    return model\n",
    "\n",
    "model = build_model((SEQ_LEN, X_seq.shape[2]))\n",
    "\n",
    "# --- Callbacks ---\n",
    "callbacks = [\n",
    "    EarlyStopping(monitor='loss', patience=2, restore_best_weights=True),\n",
    "    ModelCheckpoint(\"temp_global_model.keras\", monitor='loss', save_best_only=True)\n",
    "]\n",
    "\n",
    "# --- MLflow Run ---\n",
    "if mlflow.active_run():\n",
    "    mlflow.end_run()\n",
    "\n",
    "with mlflow.start_run():\n",
    "    model.fit(X_train, y_train, batch_size=BATCH_SIZE, epochs=EPOCHS, verbose=1, callbacks=callbacks)\n",
    "\n",
    "    y_pred = model.predict(X_test).flatten()\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "    duration = time.time() - start_time\n",
    "\n",
    "    mlflow.log_params({\n",
    "        \"model_type\": \"global_lstm\",\n",
    "        \"units\": 32,\n",
    "        \"dropout\": 0.2,\n",
    "        \"seq_len\": SEQ_LEN,\n",
    "        \"batch_size\": BATCH_SIZE,\n",
    "        \"epochs\": EPOCHS\n",
    "    })\n",
    "    mlflow.log_metrics({\n",
    "        \"mse\": mse,\n",
    "        \"mae\": mae,\n",
    "        \"r2\": r2,\n",
    "        \"train_time_sec\": duration\n",
    "    })\n",
    "\n",
    "    # --- Plot speichern ---\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.plot(y_test[:100], label=\"Actual\")\n",
    "    plt.plot(y_pred[:100], label=\"Forecast\", linestyle=\"--\")\n",
    "    plt.title(\"Forecast vs. Actual (Global LSTM)\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plot_path = \"forecast_lstm_global.png\"\n",
    "    plt.savefig(plot_path)\n",
    "    mlflow.log_artifact(plot_path)\n",
    "    plt.close()\n",
    "\n",
    "    # --- Modell und Scaler speichern ---\n",
    "    model_path = model_dir / cfg.LSTM_GLOBAL_MODEL\n",
    "    scaler_path = scaler_dir / cfg.LSTM_GLOBAL_SCALER\n",
    "    model_dir.mkdir(parents=True, exist_ok=True)\n",
    "    scaler_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    model.save(model_path)\n",
    "    joblib.dump(scaler, scaler_path)\n",
    "\n",
    "    mlflow.log_artifact(str(model_path))\n",
    "    mlflow.log_artifact(str(scaler_path))\n",
    "\n",
    "    print(f\"\\n‚úÖ Modell gespeichert: {model_path}\")\n",
    "    print(f\"‚úÖ Scaler gespeichert: {scaler_path}\")\n",
    "    print(f\"üìâ MSE: {mse:.4f} | MAE: {mae:.2f} | R¬≤: {r2:.4f}\")\n",
    "\n",
    "if mlflow.active_run():\n",
    "    mlflow.end_run()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e011e019",
   "metadata": {},
   "source": [
    "Neu¬¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda16961",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"data/preprocessed_data/train_guayas_model_ready.csv\")\n",
    "df.to_parquet(\"data/preprocessed_data/train_guayas_model_ready.parquet\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
